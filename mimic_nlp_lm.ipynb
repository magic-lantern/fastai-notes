{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using FAST.AI for Medical NLP - Step 1 Build a langauge model\n",
    "\n",
    "Exploring the MIMIC III data set medical notes.\n",
    "\n",
    "Tried working with the full dataset, but almost every training step takes many hours (~13 for initial training), predicted 14+ per epoch for fine tuning, and we need to do many epochs.\n",
    "\n",
    "Instead will try to work with just 10% sample... Not sure that will work though\n",
    "\n",
    "A few notes:\n",
    "* See https://docs.fast.ai/text.transform.html#Tokenizer for details on what various artificial tokens (e.g xxup, xxmaj, etc.) mean\n",
    "* To view nicely formatted documentation on the fastai library, run commands like: ` doc(learn.lr_find)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "import gc\n",
    "from pympler import asizeof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to verify that Torch can find and use your GPU, run the following code:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These next cells can be used to get an idea of the speed up provided by a GPU for some operations (from https://course.fast.ai/gpu_tutorial.html)\n",
    "```python\n",
    "import torch\n",
    "t_cpu = torch.rand(500,500,500)\n",
    "%timeit t_cpu @ t_cpu\n",
    "# separate cell \n",
    "t_gpu = torch.rand(500,500,500).cuda()\n",
    "%timeit t_gpu @ t_gpu\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original data set too large to work with in reasonable time due to limted GPU resources\n",
    "pct_data_sample = 0.1\n",
    "# how much to hold out for validation\n",
    "valid_pct = 0.1\n",
    "\n",
    "# pandas doesn't understand ~, so provide full path\n",
    "base_path = Path.home() / 'mimic'\n",
    "\n",
    "# files used during processing - all aggregated here\n",
    "notes_file = base_path/'noteevents.pickle'\n",
    "lm_file = 'mimic_lm.pickle' # actual file is at base_path/lm_file but due to fastai function, have to pass file name separately\n",
    "init_model_file = base_path/'mimic_fit_head'\n",
    "cycles_file = base_path/'num_iterations.pickle'\n",
    "lm_base_file = 'mimic_lm_fine_tuned_'\n",
    "enc_file = 'mimic_fine_tuned_enc'\n",
    "class_file = 'mimic_cl.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if this doesn't free memory, can restart Python kernel.\n",
    "# if that still doesn't work, try OS items mentioned here: https://docs.fast.ai/dev/gpu.html\n",
    "def release_mem():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this to see what has already been imported\n",
    "#whos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Random Number seed for repeatability; set Batch Size to control GPU memory\n",
    "\n",
    "See **\"Performance notes\"** section below for how setting batch size impacts GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "# previously used 48; worked fine but never seemed to use even half of GPU memory; 64 still on the small side\n",
    "bs=96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While parsing a CSV and converting to a dataframe is pretty fast, loading a pickle file is much faster.\n",
    "\n",
    "For load time and size comparison:\n",
    "* `NOTEEVENTS.csv` is ~ 3.8GB in size\n",
    "  ```\n",
    "  CPU times: user 51.2 s, sys: 17.6 s, total: 1min 8s\n",
    "  Wall time: 1min 47s\n",
    "  ```\n",
    "* `noteevents.pickle` is ~ 3.7 GB in size\n",
    "  ```\n",
    "  CPU times: user 2.28 s, sys: 3.98 s, total: 6.26 s\n",
    "  Wall time: 6.26 s\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading noteevent pickle file\n",
      "CPU times: user 1.56 s, sys: 3.29 s, total: 4.85 s\n",
      "Wall time: 4.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "orig_df = pd.DataFrame()\n",
    "if os.path.isfile(notes_file):\n",
    "    print('Loading noteevent pickle file')\n",
    "    orig_df = pd.read_pickle(notes_file)\n",
    "else:\n",
    "    print('Could not find noteevent pickle file; creating it')\n",
    "    # run this the first time to covert CSV to Pickle file\n",
    "    orig_df = pd.read_csv(base_path/'NOTEEVENTS.csv', low_memory=False, memory_map=True)\n",
    "    orig_df.to_pickle(notes_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to data set size and performance reasons, working with a 10% sample. Use same random see to get same results from subsequent runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = orig_df.sample(frac=pct_data_sample, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to free up some memory\n",
    "# orig_df = None\n",
    "# del orig_df\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df: 936 MB\n"
     ]
    }
   ],
   "source": [
    "print('df:', int(asizeof.asizeof(df) / 1024 / 1024), 'MB')\n",
    "#print('orig_df:', asizeof.asizeof(orig_df))\n",
    "#print('data_lm:', asizeof.asizeof(data_lm, detail=1))\n",
    "#print asizeof.asized(obj, detail=1).format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>CHARTTIME</th>\n",
       "      <th>STORETIME</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>CGID</th>\n",
       "      <th>ISERROR</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1292716</th>\n",
       "      <td>1295263</td>\n",
       "      <td>2549</td>\n",
       "      <td>159440.0</td>\n",
       "      <td>2132-04-02</td>\n",
       "      <td>2132-04-02 13:09:00</td>\n",
       "      <td>2132-04-02 13:35:00</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>18566.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CCU NSG TRANSFER SUMMARY UPDATE: RESP FAILURE\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160271</th>\n",
       "      <td>1175599</td>\n",
       "      <td>29621</td>\n",
       "      <td>190624.0</td>\n",
       "      <td>2149-02-23</td>\n",
       "      <td>2149-02-23 03:27:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Radiology</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[**2149-2-23**] 3:27 AM\\n CHEST (PORTABLE AP) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1549380</th>\n",
       "      <td>1555118</td>\n",
       "      <td>22384</td>\n",
       "      <td>142591.0</td>\n",
       "      <td>2185-03-26</td>\n",
       "      <td>2185-03-26 17:58:00</td>\n",
       "      <td>2185-03-26 18:01:00</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>16985.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Respiratory Care\\nPt remains intubated (#7.5 E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7474</th>\n",
       "      <td>5743</td>\n",
       "      <td>690</td>\n",
       "      <td>152820.0</td>\n",
       "      <td>2182-09-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2182-9-12**]       Dischar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014768</th>\n",
       "      <td>2023163</td>\n",
       "      <td>25560</td>\n",
       "      <td>156143.0</td>\n",
       "      <td>2154-11-18</td>\n",
       "      <td>2154-11-18 10:44:00</td>\n",
       "      <td>2154-11-18 17:08:00</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>16888.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Neonatology\\nOn exam pink active non-dysmorphi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ROW_ID  SUBJECT_ID   HADM_ID   CHARTDATE            CHARTTIME  \\\n",
       "1292716  1295263        2549  159440.0  2132-04-02  2132-04-02 13:09:00   \n",
       "1160271  1175599       29621  190624.0  2149-02-23  2149-02-23 03:27:00   \n",
       "1549380  1555118       22384  142591.0  2185-03-26  2185-03-26 17:58:00   \n",
       "7474        5743         690  152820.0  2182-09-14                  NaN   \n",
       "2014768  2023163       25560  156143.0  2154-11-18  2154-11-18 10:44:00   \n",
       "\n",
       "                   STORETIME           CATEGORY          DESCRIPTION     CGID  \\\n",
       "1292716  2132-04-02 13:35:00      Nursing/other               Report  18566.0   \n",
       "1160271                  NaN          Radiology  CHEST (PORTABLE AP)      NaN   \n",
       "1549380  2185-03-26 18:01:00      Nursing/other               Report  16985.0   \n",
       "7474                     NaN  Discharge summary               Report      NaN   \n",
       "2014768  2154-11-18 17:08:00      Nursing/other               Report  16888.0   \n",
       "\n",
       "         ISERROR                                               TEXT  \n",
       "1292716      NaN  CCU NSG TRANSFER SUMMARY UPDATE: RESP FAILURE\\...  \n",
       "1160271      NaN  [**2149-2-23**] 3:27 AM\\n CHEST (PORTABLE AP) ...  \n",
       "1549380      NaN  Respiratory Care\\nPt remains intubated (#7.5 E...  \n",
       "7474         NaN  Admission Date:  [**2182-9-12**]       Dischar...  \n",
       "2014768      NaN  Neonatology\\nOn exam pink active non-dysmorphi...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ROW_ID           int64\n",
       "SUBJECT_ID       int64\n",
       "HADM_ID        float64\n",
       "CHARTDATE       object\n",
       "CHARTTIME       object\n",
       "STORETIME       object\n",
       "CATEGORY        object\n",
       "DESCRIPTION     object\n",
       "CGID           float64\n",
       "ISERROR        float64\n",
       "TEXT            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208318, 11)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to build initial version of language model; If running with full dataset, requires a **LOT** of RAM; using a **LOT** of CPU helps it to happen quickly as well\n",
    "\n",
    "Questions:\n",
    "\n",
    "* why does this only seem to use CPU? (applies to both both textclasdatabunch and textlist)\n",
    "* for 100% of the mimic noteevents data:\n",
    "  * run out of memory at 32 GB, error at 52 GB, trying 72GB now... got down to only 440MB free; if crash again, increase memory\n",
    "  * now at 20vCPU and 128GB RAM; ok up to 93%; got down to 22GB available\n",
    "  * succeeded with 20CPU and 128GB RAM...\n",
    "* try smaller batch size? will that reduce memory requirements?\n",
    "* with 10% dataset sample, it seems I could get by with perhaps 32GB system RAM\n",
    "\n",
    "For comparison:\n",
    "* 10% language model is ~ 1.2 GB in size\n",
    "  * Time to load existing language model:\n",
    "    ```\n",
    "    CPU times: user 3.29 s, sys: 844 ms, total: 4.14 s\n",
    "    Wall time: 12.6 s\n",
    "    ```\n",
    "  * Time to build language model:\n",
    "    ```\n",
    "    CPU times: user 36.9 s, sys: 8.56 s, total: 45.4 s\n",
    "    Wall time: 3min 27s\n",
    "    ```\n",
    "* 100% language model is...\n",
    "  * Time to load existing language model:\n",
    "  * Time to build language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading existing language model\n",
      "CPU times: user 2.55 s, sys: 1.57 s, total: 4.12 s\n",
      "Wall time: 4.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tmpfile = base_path/lm_file\n",
    "\n",
    "if os.path.isfile(tmpfile):\n",
    "    print('loading existing language model')\n",
    "    data_lm = load_data(base_path, lm_file, bs=bs)\n",
    "else:\n",
    "    print('creating new language model')\n",
    "    data_lm = (TextList.from_df(df, 'texts.csv', cols='TEXT')\n",
    "               #df has several columns; actual text is in column TEXT\n",
    "               .split_by_rand_pct(valid_pct=valid_pct, seed=seed)\n",
    "               #We randomly split and keep 10% for validation\n",
    "               .label_for_lm()\n",
    "               #We want to do a language model so we label accordingly\n",
    "               .databunch(bs=bs))\n",
    "    data_lm.save(tmpfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If need to view more data, run appropriate line to make display wider/show more columns...\n",
    "```python\n",
    "# default 20\n",
    "pd.get_option('display.max_columns')\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.max_columns', None) # show all\n",
    "# default 50\n",
    "pd.get_option('display.max_colwidth')\n",
    "pd.set_option('display.max_colwidth', -1) # show all\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>pacs . xxmaj bp went back to 150 / 80 . xxup hr varies between 80s to low 90s at rest up to 1-teens with activity . xxup bp varies more widely between 1-teens / 70s at rest up to 170 / 90s with activity . xxmaj she continues on dilt 90 mg po qid . xxmaj she was xxup k+ replaced today . \\n  xxup resp : xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>cl 87 , xxup hco3 31 \\n  xxup wbc 10.9 , xxmaj hct 35.4 , xxmaj plt 327 \\n  xxmaj blood cx pending \\n \\n  a / p : 20 do male infant at xxup cga of 27 weeks . xxmaj labile on vent and requiring somewhat increased support for developing xxup cld . xxmaj intermittent murmur but echo on [ * * 4 - 9 *</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spine , xxup trauma ( xxup with xxup flex &amp; xxup ext ) ; t - l xxup spine 3 ' xxup film xxup ap &amp; xxup lat xxmaj clip # [ * * xxmaj clip xxmaj number ( xxmaj radiology ) xxunk * * ] \\n  xxmaj reason : s / p xxup mvc , s / p xxup mvc \\n  xxrep 78 _ \\n  [</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>xxmaj chest : ( xxmaj expansion : xxmaj symmetric ) , ( xxmaj breath xxmaj sounds : xxmaj crackles : \\n  slight at bases anteriorly , xxmaj no(t ) xxmaj wheezes : , xxmaj no(t ) xxmaj rhonchorous : ) \\n  xxmaj abdominal : xxmaj soft , xxmaj non - tender , xxmaj bowel sounds present , mildly distended but \\n  soft \\n  xxmaj extremities :</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>views , xxup pa xxup and xxup lateral \\n \\n  xxmaj history of xxup cabg . \\n \\n  xxmaj status post xxup cabg . xxup picc line is in mid xxup svc . xxmaj the lungs are clear . xxmaj no \\n  pneumothorax or pleural effusion . xxmaj there is cardiomegaly but no evidence for \\n  xxup chf . \\n \\n \\n  xxbos [ *</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lm.show_batch()\n",
    "# how to look at original version of text\n",
    "#df[df['TEXT'].str.contains('being paralyzed were discussed', case=False)].TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as of June 2019, this automatically loads and initializes the model based on WT103 from\n",
    "# https://s3.amazonaws.com/fast-ai-modelzoo/wt103-fwd.tgz; will auto download if not already on disk\n",
    "learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Learning rate graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot(skip_end=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial model training\n",
    "\n",
    "Time to run:\n",
    "\n",
    "* Full data set took about 13 hours using the Nvidia P1000\n",
    "* Full data set was predicted to take about 25 hours with the T4\n",
    "* 10% data took about 1 hour (1:08) using the Nvidia P1000\n",
    "* 10% data is predicted to take about 2.5 hour (actual 2:42) using the Nvidia GTX 1060\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 1:04:35 <p><table style='width:375px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "    <th>time</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>0</th>\n",
       "    <th>2.444923</th>\n",
       "    <th>2.261303</th>\n",
       "    <th>0.558605</th>\n",
       "    <th>1:04:35</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated new learner\n"
     ]
    }
   ],
   "source": [
    "# no idea how long nor how much resources this will take\n",
    "# not sure 1e-2 is the right learning rate; maybe 1e-1 or between 1e-2 and 1e-1\n",
    "# using t4\n",
    "# progress bar says this will take around 24 hours... ran for about 52 minutes\n",
    "# gpustat/nvidia-smi indicates currently only using about 5GB of GPU RAM\n",
    "# using p100\n",
    "# progress bar says this will take around 12 hours; took 13:16\n",
    "# at start GPU using about 5GB RAM\n",
    "# after about 8 hours GPU using about 7.5GB RAM.\n",
    "# looks like I could increase batch size...\n",
    "# with bs=64, still only seems to be using about 7GB GPU RAM after running for 15 minutes. \n",
    "# will check after a bit, but likely can increase batch size further\n",
    "#\n",
    "# note about number of epochs/cycle length: Using a value of 1 does a rapid increase and\n",
    "# decrease of learning rate and end result gets almost the save result as 2 but in half\n",
    "# the time\n",
    "if os.path.isfile(str(init_model_file) + '.pth'):\n",
    "    learn.load(init_model_file)\n",
    "    print('loaded learner')\n",
    "else:\n",
    "    learn.fit_one_cycle(1, 5e-2, moms=(0.8,0.7))\n",
    "    learn.save(init_model_file)\n",
    "    print('generated new learner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "continue from initial training - reload in case just want to continue processing from here.\n",
    "\n",
    "As an FYI pytorch automatically appends .pth to the filename, you cannot provide it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)\n",
    "#learn.load(init_model_file)\n",
    "#print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>xxbos 88 m with h / o xxup dm2 , xxup htn , hyperlipidemia , xxup chf , s /</td>\n",
       "      <td>p xxup cva , fungal bladder \\n  mass and recurrent utis presents from nursing home 3x vomiting this xxup</td>\n",
       "      <td>p xxup cabg , xxup \\n  ca and , \\n  \\n  , with xxup home . /</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[ * * 5 - 27 * * ] with negative legionella and xxup pcp \\n  [ * *</td>\n",
       "      <td>xxmaj name9 ( xxup pre ) 6930 * * ] with pt 's uncle xxmaj dr. [ * * xxmaj</td>\n",
       "      <td>name ( xxup pre ) * * * ] xxup xxup . xxup with pt [ * * xxmaj last</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>clear to coarse throughout . xxmaj suctioned for moderate amounts thick white secretions . xxmaj remains on xxmaj pressure support</td>\n",
       "      <td>ventilation 5 / 5 / 40 % maintaining sats 100 % . xxup rr in 20 's . \\n \\n</td>\n",
       "      <td>on xxup ventilator xxmaj . . . 5 . 5 % . xxup &gt; % . xxmaj o2 30 \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxup o2 xxmaj delivery xxmaj device : xxmaj endotracheal tube \\n  xxmaj ventilator mode : xxup cmv / xxup</td>\n",
       "      <td>assist / autoflow \\n  xxmaj vt ( xxmaj set ) : 550 ( 550 - 550 ) ml \\n</td>\n",
       "      <td>assist / autoflow \\n  xxmaj vt ( xxmaj set ) : 500 ( 550 - 550 ) ml \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>/ ml \\n  [ * * 2148 - 5 - 1 * * ] 01:32 xxup am \\n</td>\n",
       "      <td>xxup wbc \\n  10.1 k / ul \\n  [ * * 2148 - 5 - 1 * *</td>\n",
       "      <td>fever \\n  xxup \\n  / ul \\n  [ * * 2185 - 9 - 15 * *</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cycles_file, 'wb') as f:\n",
    "    pickle.dump(8, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model has been trained for 8 epochs already\n"
     ]
    }
   ],
   "source": [
    "prev_cycles = 0\n",
    "\n",
    "if os.path.isfile(cycles_file):\n",
    "    with open(cycles_file, 'rb') as f:\n",
    "        prev_cycles = pickle.load(f)\n",
    "print('This model has been trained for', prev_cycles, 'epochs already')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp_files = glob.glob(str(base_path/'*_auto_*'))\n",
    "#if len(training_files) > 0:\n",
    "rfiles = glob.glob(str(base_path/'*_auto_*'))\n",
    "rfiles.sort()\n",
    "if (len(rfiles) > 0):\n",
    "    print('There are pre-existing automatic save states. Remove these files if no longer needed.')\n",
    "for f in rfiles:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Now fine tune language model\n",
    "\n",
    "Performance notes w/P100 GPU:\n",
    "\n",
    "* at batch size of 128 takes about 1:14:00 per epoch; GPU usage is about 14GB; RAM usage is about 10GB\n",
    "* at batch size of 96 takes about 1:17:00 per epoch; GPU usage is about  9GB; RAM usage is about 10GB\n",
    "* at batch size of 48 takes about 1:30:00 per epoch; GPU usage is about  5GB; RAM usage is about 10GB\n",
    "\n",
    "With `learn.fit_one_cycle(8, 5e-3, moms=(0.8,0.7))` (8 cycles)\n",
    "* gets from about 62.7% accuracy to 67.6% accuracy\n",
    "* Total time: 9:54:16\n",
    "\n",
    "\n",
    "    epoch \ttrain_loss \tvalid_loss \taccuracy \ttime\n",
    "    0 \t1.926960 \t1.832659 \t0.627496 \t1:14:14\n",
    "    1 \t1.808083 \t1.755725 \t0.637424 \t1:14:15\n",
    "    2 \t1.747903 \t1.697741 \t0.645431 \t1:14:15\n",
    "    3 \t1.714081 \t1.652703 \t0.652703 \t1:14:19\n",
    "    4 \t1.637801 \t1.602961 \t0.660170 \t1:14:15\n",
    "    5 \t1.596906 \t1.553225 \t0.668557 \t1:14:14\n",
    "    6 \t1.572020 \t1.519172 \t0.674477 \t1:14:26\n",
    "    7 \t1.517364 \t1.510010 \t0.676342 \t1:14:14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_learner_load(lf):\n",
    "    if os.path.isfile(str(lf) + '.pth'):\n",
    "        learn.load(lf)\n",
    "        print('loaded existing learner from', str(lf))\n",
    "    else:\n",
    "        # should not continue as could not find specified file\n",
    "        print('existing learner file (', lf, ') not found, cannot continue')\n",
    "        print('previous epoch may have only partially completed')\n",
    "        print(' --- try updating prev_cycles to match or copy file to correct name.')\n",
    "        assert(False)\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No auto save files exist from interupted training.\n",
      "Starting training with base language model\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='8' class='' max='15', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      53.33% [8/15 9:46:05<8:32:49]\n",
       "    </div>\n",
       "    \n",
       "<table style='width:375px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "    <th>time</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>0</th>\n",
       "    <th>1.901465</th>\n",
       "    <th>1.825830</th>\n",
       "    <th>0.630896</th>\n",
       "    <th>1:13:11</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>1.739823</th>\n",
       "    <th>1.734608</th>\n",
       "    <th>0.642237</th>\n",
       "    <th>1:13:13</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>2</th>\n",
       "    <th>1.765395</th>\n",
       "    <th>1.743577</th>\n",
       "    <th>0.639471</th>\n",
       "    <th>1:13:12</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>3</th>\n",
       "    <th>1.805623</th>\n",
       "    <th>1.752152</th>\n",
       "    <th>0.638325</th>\n",
       "    <th>1:13:22</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>4</th>\n",
       "    <th>1.804578</th>\n",
       "    <th>1.731174</th>\n",
       "    <th>0.641581</th>\n",
       "    <th>1:13:13</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>5</th>\n",
       "    <th>1.747588</th>\n",
       "    <th>1.705513</th>\n",
       "    <th>0.645204</th>\n",
       "    <th>1:13:15</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>6</th>\n",
       "    <th>1.704296</th>\n",
       "    <th>1.677969</th>\n",
       "    <th>0.649215</th>\n",
       "    <th>1:13:15</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>7</th>\n",
       "    <th>1.682875</th>\n",
       "    <th>1.650206</th>\n",
       "    <th>0.653763</th>\n",
       "    <th>1:13:13</th>\n",
       "  </tr>\n",
       "</table>\n",
       "\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='progress-bar-interrupted' max='13475', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      Interrupted\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-d67c0acd2c38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStoppingCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0025\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                     ],\n\u001b[0;32m---> 74\u001b[0;31m                     start_epoch=start_epoch)\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlm_base_file\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_cycles\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_cycles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0mlearner_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_path\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/fastai/train.py\u001b[0m in \u001b[0;36mfit_one_cycle\u001b[0;34m(learn, cyc_len, max_lr, moms, div_factor, pct_start, final_div, wd, callbacks, tot_epochs, start_epoch)\u001b[0m\n\u001b[1;32m     20\u001b[0m     callbacks.append(OneCycleScheduler(learn, max_lr, moms=moms, div_factor=div_factor, pct_start=pct_start,\n\u001b[1;32m     21\u001b[0m                                        final_div=final_div, tot_epochs=tot_epochs, start_epoch=start_epoch))\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcyc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mLearner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_it\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_div\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callback_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn, callbacks, metrics)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(model, xb, yb, loss_func, opt, cb_handler)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mskip_bwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_backward_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mskip_bwd\u001b[0m\u001b[0;34m:\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_backward_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/fastai/callback.py\u001b[0m in \u001b[0;36mon_backward_begin\u001b[0;34m(self, loss)\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_backward_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;34m\"Handle gradient calculation on `loss`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmoothener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'last_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'smooth_loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmoothener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmooth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'backward_begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall_mets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# if want to continue training existing model, set to True\n",
    "# if want to start fresh from the initialized language model, set to False\n",
    "# also, make sure to remove any previously created saved states before changing\n",
    "# flag back to continue\n",
    "continue_flag = False\n",
    "# Resume interrupted training\n",
    "resume_flag = True\n",
    "########################################################\n",
    "learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)\n",
    "\n",
    "########################################################\n",
    "# set this to how many cycles you want to run\n",
    "num_cycles = 10\n",
    "########################################################\n",
    "lm_base_file = 'mimic_lm_fine_tuned_'\n",
    "if continue_flag:\n",
    "    if os.path.isfile(cycles_file):\n",
    "        with open(cycles_file, 'rb') as f:\n",
    "            prev_cycles = pickle.load(f)\n",
    "        print('This model has been trained for', prev_cycles, 'epochs already')  \n",
    "else:\n",
    "    prev_cycles = 0\n",
    "\n",
    "file = lm_base_file + str(prev_cycles)\n",
    "learner_file = base_path/file\n",
    "callback_save_file = str(learner_file) + '_auto'\n",
    "fn_pattern = callback_save_file + '*'\n",
    "\n",
    "\n",
    "# for one cycle learning with learning rate annealing - where to resume from\n",
    "start_epoch = 0\n",
    "\n",
    "if resume_flag:\n",
    "    training_files = glob.glob(str(base_path/fn_pattern))\n",
    "    if len(training_files) > 0:\n",
    "        training_files.sort()\n",
    "        completed_cycles = int(re.split('_|\\.', training_files[-1])[-2])\n",
    "        if completed_cycles < (num_cycles - 1):\n",
    "            # need to load the last file\n",
    "            print('Previous training cycle of', num_cycles, 'did not complete; finished',\n",
    "                  completed_cycles + 1, 'cycles. Loading last save...')\n",
    "            # load just filename, drop extension of .pth as that is automatically appended inside load function\n",
    "            learn.load(os.path.splitext(training_files[-1])[0])\n",
    "            start_epoch = completed_cycles + 1\n",
    "        else:\n",
    "            print('Previous training cycle of', num_cycles, 'completed fully.')\n",
    "            learn = custom_learner_load(learner_file)\n",
    "    else:\n",
    "        print('No auto save files exist from interupted training.')\n",
    "        if continue_flag:\n",
    "            learn = custom_learner_load(learner_file)\n",
    "        else:\n",
    "            print('Starting training with base language model')\n",
    "else:\n",
    "    if continue_flag:\n",
    "        learn = custom_learner_load(learner_file)\n",
    "    else:\n",
    "        print('Starting training with base language model')\n",
    "    # remove any auto saves\n",
    "    training_files = glob.glob(str(base_path/fn_pattern))\n",
    "    if len(training_files) > 0:\n",
    "        for f in training_files:\n",
    "            print('Deleting', f)\n",
    "            os.remove(f)\n",
    "\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(num_cycles, 5e-3, moms=(0.8,0.7),\n",
    "                    callbacks=[\n",
    "                        callbacks.SaveModelCallback(learn, every='epoch', monitor='accuracy', name=callback_save_file),\n",
    "                        # CSVLogger only logs when num_cycles are complete\n",
    "                        callbacks.CSVLogger(learn, filename='mimic_lm_fine_tune_history', append=True),\n",
    "                        callbacks.EarlyStoppingCallback(learn, monitor='accuracy', min_delta=0.0025, patience=5)\n",
    "                    ],\n",
    "                    start_epoch=start_epoch)\n",
    "file = lm_base_file + str(prev_cycles + num_cycles)\n",
    "learner_file = base_path/file\n",
    "learn.save(learner_file)\n",
    "\n",
    "with open(cycles_file, 'wb') as f:\n",
    "    pickle.dump(num_cycles + prev_cycles, f)\n",
    "release_mem()\n",
    "    \n",
    "print('completed', num_cycles, 'new training epochs')\n",
    "print('completed', num_cycles + prev_cycles, 'total training epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate different learning rates.\n",
    "\n",
    "Use this block of code to compare how well a few different learning rates work\n",
    "\n",
    "Found that `5e-3` works best with `learn.unfreeze()`\n",
    "\n",
    "```python\n",
    "num_cycles = 4\n",
    "prev_cycles = 4\n",
    "\n",
    "for lr in [1e-3, 5e-3, 1e-2, 5e-2, 1e-1]:\n",
    "    print('now testing with multiple epochs and learning rate of', lr)\n",
    "    print('This model has been trained for', prev_cycles, 'epochs already')    \n",
    "    file = lm_base_file + str(prev_cycles)\n",
    "    learner_file = base_path/file\n",
    "    learn.load(learner_file)\n",
    "    learn.unfreeze()\n",
    "    print('loaded existing learner from', str(learner_file))\n",
    "\n",
    "\n",
    "    learn.fit_one_cycle(num_cycles, lr, moms=(0.8,0.7))\n",
    "    file = lm_base_file + str(prev_cycles + num_cycles + 1)\n",
    "    learner_file = base_path/file\n",
    "    learn.save(learner_file)\n",
    "    release_mem()\n",
    "\n",
    "    print('completed', num_cycles, 'new training epochs')\n",
    "    print('completed', num_cycles + prev_cycles, 'total training epochs')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the language generation capabilities of this model (not the point, but is interesting)\n",
    "TEXT = \"For confirmation, she underwent CTA of the lung which was negative for pulmonary embolism\"\n",
    "N_WORDS = 40\n",
    "N_SENTENCES = 2\n",
    "print(\"\\n\".join(learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder(enc_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the encoder:\n",
    "\n",
    "```python\n",
    "learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)\n",
    "learn.load_encoder(enc_file)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
