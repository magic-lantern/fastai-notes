{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using FAST.AI for Medical NLP\n",
    "\n",
    "Exploring the MIMIC III data set medical notes.\n",
    "\n",
    "Tried working with the full dataset, but almost every training step takes many hours (~13 for initial training), predicted 14+ per epoch for fine tuning, and we need to do many epochs.\n",
    "\n",
    "Instead will try to work with just 10% sample... Not sure that will work though\n",
    "\n",
    "A few notes:\n",
    "* See https://docs.fast.ai/text.transform.html#Tokenizer for details on what various artificial tokens (e.g xxup, xxmaj, etc.) mean\n",
    "* To view nicely formatted documentation on the fastai library, run commands like: ` doc(learn.lr_find)`\n",
    "\n",
    "### To Do:\n",
    "* need to evalate how changing the learning rate would alter training time\n",
    "* need to evalate how changing the learning rate would alter accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "import gc\n",
    "from pympler import asizeof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to verify that Torch can find and use your GPU, run the following code:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These next cells can be used to get an idea of the speed up provided by a GPU for some operations (from https://course.fast.ai/gpu_tutorial.html)\n",
    "```python\n",
    "import torch\n",
    "t_cpu = torch.rand(500,500,500)\n",
    "%timeit t_cpu @ t_cpu\n",
    "# separate cell \n",
    "t_gpu = torch.rand(500,500,500).cuda()\n",
    "%timeit t_gpu @ t_gpu\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original data set too large to work with in reasonable time due to limted GPU resources\n",
    "pct_data_sample = 0.01\n",
    "# how much to hold out for validation\n",
    "valid_pct = 0.1\n",
    "\n",
    "# pandas doesn't understand ~, so provide full path\n",
    "base_path = Path.home() / 'mimic'\n",
    "\n",
    "# files used during processing - all aggregated here\n",
    "notes_file = base_path/'noteevents.pickle'\n",
    "lm_file = 'mimic_lm.pickle' # actual file is at base_path/lm_file but due to fastai function, have to pass file name separately\n",
    "init_model_file = base_path/'mimic_fit_head'\n",
    "cycles_file = base_path/'num_iterations.pickle'\n",
    "lm_base_file = 'mimic_lm_fine_tuned_'\n",
    "enc_file = 'mimic_fine_tuned_enc'\n",
    "class_file = 'mimic_cl.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if this doesn't free memory, can restart Python kernel.\n",
    "# if that still doesn't work, try OS items mentioned here: https://docs.fast.ai/dev/gpu.html\n",
    "def release_mem():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this to see what has already been imported\n",
    "#whos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Random Number seed for repeatability; set Batch Size to control GPU memory\n",
    "\n",
    "See **\"Performance notes\"** section below for how setting batch size impacts GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "# previously used 48; worked fine but never seemed to use even half of GPU memory; 64 still on the small side\n",
    "bs=96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While parsing a CSV and converting to a dataframe is pretty fast, loading a pickle file is much faster.\n",
    "\n",
    "For load time and size comparison:\n",
    "* `NOTEEVENTS.csv` is ~ 3.8GB in size\n",
    "  ```\n",
    "  CPU times: user 51.2 s, sys: 17.6 s, total: 1min 8s\n",
    "  Wall time: 1min 47s\n",
    "  ```\n",
    "* `noteevents.pickle` is ~ 3.7 GB in size\n",
    "  ```\n",
    "  CPU times: user 2.28 s, sys: 3.98 s, total: 6.26 s\n",
    "  Wall time: 6.26 s\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading noteevent pickle file\n",
      "CPU times: user 1.67 s, sys: 4.11 s, total: 5.78 s\n",
      "Wall time: 32.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "orig_df = pd.DataFrame()\n",
    "if os.path.isfile(notes_file):\n",
    "    print('Loading noteevent pickle file')\n",
    "    orig_df = pd.read_pickle(notes_file)\n",
    "else:\n",
    "    print('Could not find noteevent pickle file; creating it')\n",
    "    # run this the first time to covert CSV to Pickle file\n",
    "    orig_df = pd.read_csv(base_path/'NOTEEVENTS.csv', low_memory=False, memory_map=True)\n",
    "    orig_df.to_pickle(notes_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to data set size and performance reasons, working with a 10% sample. Use same random see to get same results from subsequent runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = orig_df.sample(frac=pct_data_sample, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to free up some memory\n",
    "# orig_df = None\n",
    "# del orig_df\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df: 93 MB\n"
     ]
    }
   ],
   "source": [
    "print('df:', int(asizeof.asizeof(df) / 1024 / 1024), 'MB')\n",
    "#print('orig_df:', asizeof.asizeof(orig_df))\n",
    "#print('data_lm:', asizeof.asizeof(data_lm, detail=1))\n",
    "#print asizeof.asized(obj, detail=1).format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>CHARTTIME</th>\n",
       "      <th>STORETIME</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>CGID</th>\n",
       "      <th>ISERROR</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1292716</th>\n",
       "      <td>1295263</td>\n",
       "      <td>2549</td>\n",
       "      <td>159440.0</td>\n",
       "      <td>2132-04-02</td>\n",
       "      <td>2132-04-02 13:09:00</td>\n",
       "      <td>2132-04-02 13:35:00</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>18566.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CCU NSG TRANSFER SUMMARY UPDATE: RESP FAILURE\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160271</th>\n",
       "      <td>1175599</td>\n",
       "      <td>29621</td>\n",
       "      <td>190624.0</td>\n",
       "      <td>2149-02-23</td>\n",
       "      <td>2149-02-23 03:27:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Radiology</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[**2149-2-23**] 3:27 AM\\n CHEST (PORTABLE AP) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1549380</th>\n",
       "      <td>1555118</td>\n",
       "      <td>22384</td>\n",
       "      <td>142591.0</td>\n",
       "      <td>2185-03-26</td>\n",
       "      <td>2185-03-26 17:58:00</td>\n",
       "      <td>2185-03-26 18:01:00</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>16985.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Respiratory Care\\nPt remains intubated (#7.5 E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7474</th>\n",
       "      <td>5743</td>\n",
       "      <td>690</td>\n",
       "      <td>152820.0</td>\n",
       "      <td>2182-09-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2182-9-12**]       Dischar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014768</th>\n",
       "      <td>2023163</td>\n",
       "      <td>25560</td>\n",
       "      <td>156143.0</td>\n",
       "      <td>2154-11-18</td>\n",
       "      <td>2154-11-18 10:44:00</td>\n",
       "      <td>2154-11-18 17:08:00</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>16888.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Neonatology\\nOn exam pink active non-dysmorphi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ROW_ID  SUBJECT_ID   HADM_ID   CHARTDATE            CHARTTIME  \\\n",
       "1292716  1295263        2549  159440.0  2132-04-02  2132-04-02 13:09:00   \n",
       "1160271  1175599       29621  190624.0  2149-02-23  2149-02-23 03:27:00   \n",
       "1549380  1555118       22384  142591.0  2185-03-26  2185-03-26 17:58:00   \n",
       "7474        5743         690  152820.0  2182-09-14                  NaN   \n",
       "2014768  2023163       25560  156143.0  2154-11-18  2154-11-18 10:44:00   \n",
       "\n",
       "                   STORETIME           CATEGORY          DESCRIPTION     CGID  \\\n",
       "1292716  2132-04-02 13:35:00      Nursing/other               Report  18566.0   \n",
       "1160271                  NaN          Radiology  CHEST (PORTABLE AP)      NaN   \n",
       "1549380  2185-03-26 18:01:00      Nursing/other               Report  16985.0   \n",
       "7474                     NaN  Discharge summary               Report      NaN   \n",
       "2014768  2154-11-18 17:08:00      Nursing/other               Report  16888.0   \n",
       "\n",
       "         ISERROR                                               TEXT  \n",
       "1292716      NaN  CCU NSG TRANSFER SUMMARY UPDATE: RESP FAILURE\\...  \n",
       "1160271      NaN  [**2149-2-23**] 3:27 AM\\n CHEST (PORTABLE AP) ...  \n",
       "1549380      NaN  Respiratory Care\\nPt remains intubated (#7.5 E...  \n",
       "7474         NaN  Admission Date:  [**2182-9-12**]       Dischar...  \n",
       "2014768      NaN  Neonatology\\nOn exam pink active non-dysmorphi...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ROW_ID           int64\n",
       "SUBJECT_ID       int64\n",
       "HADM_ID        float64\n",
       "CHARTDATE       object\n",
       "CHARTTIME       object\n",
       "STORETIME       object\n",
       "CATEGORY        object\n",
       "DESCRIPTION     object\n",
       "CGID           float64\n",
       "ISERROR        float64\n",
       "TEXT            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20832, 11)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train and test sets; using same random seed so subsequent runs will generate same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 1./3\n",
    "train, test = train_test_split(df, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13888, 11)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6944, 11)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to build initial version of language model; If running with full dataset, requires a **LOT** of RAM; using a **LOT** of CPU helps it to happen quickly as well\n",
    "\n",
    "Questions:\n",
    "\n",
    "* why does this only seem to use CPU? (applies to both both textclasdatabunch and textlist)\n",
    "* for 100% of the mimic noteevents data:\n",
    "  * run out of memory at 32 GB, error at 52 GB, trying 72GB now... got down to only 440MB free; if crash again, increase memory\n",
    "  * now at 20vCPU and 128GB RAM; ok up to 93%; got down to 22GB available\n",
    "  * succeeded with 20CPU and 128GB RAM...\n",
    "* try smaller batch size? will that reduce memory requirements?\n",
    "* with 10% dataset sample, it seems I could get by with perhaps 32GB system RAM\n",
    "\n",
    "For comparison:\n",
    "* 10% language model is ~ 1.2 GB in size\n",
    "  * Time to load existing language model:\n",
    "    ```\n",
    "    CPU times: user 3.29 s, sys: 844 ms, total: 4.14 s\n",
    "    Wall time: 12.6 s\n",
    "    ```\n",
    "  * Time to build language model:\n",
    "    ```\n",
    "    CPU times: user 36.9 s, sys: 8.56 s, total: 45.4 s\n",
    "    Wall time: 3min 27s\n",
    "    ```\n",
    "* 100% language model is...\n",
    "  * Time to load existing language model:\n",
    "  * Time to build language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading existing language model\n",
      "CPU times: user 308 ms, sys: 164 ms, total: 472 ms\n",
      "Wall time: 1.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tmpfile = base_path/lm_file\n",
    "\n",
    "if os.path.isfile(tmpfile):\n",
    "    print('loading existing language model')\n",
    "    data_lm = load_data(base_path, lm_file, bs=bs)\n",
    "else:\n",
    "    print('creating new language model')\n",
    "    data_lm = (TextList.from_df(df, 'texts.csv', cols='TEXT')\n",
    "               #df has several columns; actual text is in column TEXT\n",
    "               .split_by_rand_pct(valid_pct=valid_pct, seed=seed)\n",
    "               #We randomly split and keep 10% for validation\n",
    "               .label_for_lm()\n",
    "               #We want to do a language model so we label accordingly\n",
    "               .databunch(bs=bs))\n",
    "    data_lm.save(tmpfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If need to view more data, run appropriate line to make display wider/show more columns...\n",
    "```python\n",
    "# default 20\n",
    "pd.get_option('display.max_columns')\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.max_columns', None) # show all\n",
    "# default 50\n",
    "pd.get_option('display.max_colwidth')\n",
    "pd.set_option('display.max_colwidth', -1) # show all\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>pacs . xxmaj bp went back to 150 / 80 . xxup hr varies between 80s to low 90s at rest up to 1-teens with activity . xxup bp varies more widely between 1-teens / 70s at rest up to 170 / 90s with activity . xxmaj she continues on dilt 90 mg po qid . xxmaj she was xxup k+ replaced today . \\n  xxup resp : xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>and was beta complete . xxmaj twins born by c / xxup s. xxmaj apgars were 9,9 . xxmaj infant emerged vigorous , and crying , bulb suctioned and given tactile stim , xxmaj no xxup o2 needed . xxmaj infant placed on radiant warmer . xxmaj initial rectal temp 97.3 . xxmaj infant warmed by radiant warmer , with stable temps for remainder of this shift . xxmaj warmer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>inr:17.6 / 35.9 / 1.6 , xxup ck / xxup ckmb / \\n  xxmaj troponin - xxup xxunk / 6 / 0.03 , xxmaj differential - xxmaj neuts:79.9 % , xxmaj lymph:10.8 % , \\n  xxmaj mono:5.8 % , xxmaj eos:3.3 % , xxmaj ca++:8.0 mg / dl , xxmaj mg++:2.0 mg / dl , xxup po4:1.8 mg / dl \\n  xxmaj assessment and xxmaj plan \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>and he no longer feels that he is \\n  working hard to breathe and says that he is very comfortable . \\n  xxmaj the xxmaj cardiology xxmaj team was consulted . xxmaj the patient received \\n  three echocardiograms during hospitalization to date . xxmaj he has \\n  3 + mitral regurgitation associated with his mitral valve \\n  vegetation . xxmaj the patient was started on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[ * * xxmaj last xxmaj name ( namepattern1 ) 1782 * * ] at 11:15 a.m. on [ * * 2181 - 2 - 20 * * ] . \\n \\n  xxbos xxmaj renal failure , acute ( xxmaj acute renal failure , xxup arf ) \\n  xxmaj assessment : \\n  xxmaj pt . anuric s / p nephrectomy . \\n  xxmaj action : \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lm.show_batch()\n",
    "# how to look at original version of text\n",
    "#df[df['TEXT'].str.contains('being paralyzed were discussed', case=False)].TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as of June 2019, this automatically loads and initializes the model based on WT103 from\n",
    "# https://s3.amazonaws.com/fast-ai-modelzoo/wt103-fwd.tgz; will auto download if not already on disk\n",
    "learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Learning rate graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.lr_find()\n",
    "#learn.recorder.plot(skip_end=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial model training\n",
    "\n",
    "Time to run:\n",
    "\n",
    "* Full data set took about 13 hours using the Nvidia P1000\n",
    "* Full data set was predicted to take about 25 hours with the T4\n",
    "* 10% data took about 1 hour (1:08) using the Nvidia P1000\n",
    "* 10% data is predicted to take about 2.5 hour (actual 2:42) using the Nvidia GTX 1060\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded learner\n"
     ]
    }
   ],
   "source": [
    "# no idea how long nor how much resources this will take\n",
    "# not sure 1e-2 is the right learning rate; maybe 1e-1 or between 1e-2 and 1e-1\n",
    "# using t4\n",
    "# progress bar says this will take around 24 hours... ran for about 52 minutes\n",
    "# gpustat/nvidia-smi indicates currently only using about 5GB of GPU RAM\n",
    "# using p100\n",
    "# progress bar says this will take around 12 hours; took 13:16\n",
    "# at start GPU using about 5GB RAM\n",
    "# after about 8 hours GPU using about 7.5GB RAM.\n",
    "# looks like I could increase batch size...\n",
    "# with bs=64, still only seems to be using about 7GB GPU RAM after running for 15 minutes. \n",
    "# will check after a bit, but likely can increase batch size further\n",
    "#\n",
    "# note about number of epochs/cycle length: Using a value of 1 does a rapid increase and\n",
    "# decrease of learning rate and end result gets almost the save result as 2 but in half\n",
    "# the time\n",
    "if os.path.isfile(str(init_model_file) + '.pth'):\n",
    "    learn.load(init_model_file)\n",
    "    print('loaded learner')\n",
    "else:\n",
    "    learn.fit_one_cycle(1, 5e-2, moms=(0.8,0.7))\n",
    "    learn.save(init_model_file)\n",
    "    print('generated new learner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "continue from initial training - reload in case just want to continue processing from here.\n",
    "\n",
    "As an FYI pytorch automatically appends .pth to the filename, you cannot provide it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)\n",
    "#learn.load(init_model_file)\n",
    "#print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj resp xxmaj care xxmaj note : \\n \\n  xxmaj pt cont trached on cool mist aerosol as</td>\n",
       "      <td>per xxmaj carevue . xxmaj lung sounds coarse dim @ bases suct xxunk th pale yellow sput . xxmaj pt</td>\n",
       "      <td>tent tolerated carevue carevue . xxmaj pt sounds are suct at bases . sm thick thick yellow sput . xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5 - 8 * * ] 04:25 xxup am \\n  [ * * 2136 - 5 - 9 *</td>\n",
       "      <td>* ] 04:20 xxup am \\n  [ * * 2136 - 5 - 12 * * ] 03:19 xxup</td>\n",
       "      <td>* ] xxup xxup am \\n  xxup * * 2139 - 11 - 5 * * ] xxup xxup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>: \\n  xxmaj swan - xxmaj ganz catheter with tip at the periphery of the central pulmonary artery .</td>\n",
       "      <td>\\n  xxmaj new right internal jugular venous catheter with the tip in the lower superior \\n  vena cava</td>\n",
       "      <td>venous . xxmaj xxmaj the xxup - jugular line catheter in tip tip in the mid \\n  vena vena</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1 xxup dose xxup of xxup iv xxup lopressor . xxup standing xxup lopressor xxup dose xxup increased . \\n</td>\n",
       "      <td>xxup pt xxup alert xxup and xxup follows xxup commands xxup but xxup does xxup not xxup always xxup</td>\n",
       "      <td>xxmaj xxunk xxup has xxup and xxup oriented xxup commands . and xxup not xxup not xxup want xxup to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>. \\n  xxup issues : \\n  - xxup tbm , s / p xxunk and posterior tracheal splinting</td>\n",
       "      <td>via r \\n  thoracotomy [ * * 1 - 23 * * ] \\n  - subcutaneous emphysema \\n</td>\n",
       "      <td>, xxup \\n  side . * * xxmaj - 2 * * ] . xxmaj xxmaj \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cycles_file, 'wb') as f:\n",
    "    pickle.dump(8, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model has been trained for 8 epochs already\n"
     ]
    }
   ],
   "source": [
    "prev_cycles = 0\n",
    "\n",
    "if os.path.isfile(cycles_file):\n",
    "    with open(cycles_file, 'rb') as f:\n",
    "        prev_cycles = pickle.load(f)\n",
    "print('This model has been trained for', prev_cycles, 'epochs already')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are pre-existing automatic save states. Remove these files if no longer needed.\n",
      "/home/jupyter/mimic/mimic_lm_fine_tuned_0_auto_0.pth\n",
      "/home/jupyter/mimic/mimic_lm_fine_tuned_0_auto_1.pth\n",
      "/home/jupyter/mimic/mimic_lm_fine_tuned_2_auto_0.pth\n",
      "/home/jupyter/mimic/mimic_lm_fine_tuned_2_auto_1.pth\n",
      "/home/jupyter/mimic/mimic_lm_fine_tuned_4_auto_0.pth\n",
      "/home/jupyter/mimic/mimic_lm_fine_tuned_4_auto_1.pth\n",
      "/home/jupyter/mimic/mimic_lm_fine_tuned__auto_0.pth\n",
      "/home/jupyter/mimic/mimic_lm_fine_tuned__auto_1.pth\n"
     ]
    }
   ],
   "source": [
    "#temp_files = glob.glob(str(base_path/'*_auto_*'))\n",
    "#if len(training_files) > 0:\n",
    "rfiles = glob.glob(str(base_path/'*_auto_*'))\n",
    "rfiles.sort()\n",
    "if (len(rfiles) > 0):\n",
    "    print('There are pre-existing automatic save states. Remove these files if no longer needed.')\n",
    "for f in rfiles:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Now fine tune language model\n",
    "\n",
    "Performance notes w/P100 GPU:\n",
    "\n",
    "* at batch size of 128 takes about 1:14:00 per epoch; GPU usage is about 14GB; RAM usage is about 10GB\n",
    "* at batch size of 96 takes about 1:17:00 per epoch; GPU usage is about  9GB; RAM usage is about 10GB\n",
    "* at batch size of 48 takes about 1:30:00 per epoch; GPU usage is about  5GB; RAM usage is about 10GB\n",
    "\n",
    "With `learn.fit_one_cycle(8, 5e-3, moms=(0.8,0.7))` (8 cycles)\n",
    "* gets from about 62.7% accuracy to 67.6% accuracy\n",
    "* Total time: 9:54:16\n",
    "\n",
    "\n",
    "    epoch \ttrain_loss \tvalid_loss \taccuracy \ttime\n",
    "    0 \t1.926960 \t1.832659 \t0.627496 \t1:14:14\n",
    "    1 \t1.808083 \t1.755725 \t0.637424 \t1:14:15\n",
    "    2 \t1.747903 \t1.697741 \t0.645431 \t1:14:15\n",
    "    3 \t1.714081 \t1.652703 \t0.652703 \t1:14:19\n",
    "    4 \t1.637801 \t1.602961 \t0.660170 \t1:14:15\n",
    "    5 \t1.596906 \t1.553225 \t0.668557 \t1:14:14\n",
    "    6 \t1.572020 \t1.519172 \t0.674477 \t1:14:26\n",
    "    7 \t1.517364 \t1.510010 \t0.676342 \t1:14:14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_learner_load(lf):\n",
    "    if os.path.isfile(str(lf) + '.pth'):\n",
    "        learn.load(lf)\n",
    "        print('loaded existing learner from', str(lf))\n",
    "    else:\n",
    "        # should not continue as could not find specified file\n",
    "        print('existing learner file (', lf, ') not found, cannot continue')\n",
    "        print('previous epoch may have only partially completed')\n",
    "        print(' --- try updating prev_cycles to match or copy file to correct name.')\n",
    "        assert(False)\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model has been trained for 2 epochs already\n",
      "No auto save files exist from interupted training.\n",
      "loaded existing learner from /home/jupyter/mimic/mimic_lm_fine_tuned_2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Total time: 10:39 <p><table style='width:375px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "    <th>time</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>0</th>\n",
       "    <th>1.869730</th>\n",
       "    <th>1.929068</th>\n",
       "    <th>0.607264</th>\n",
       "    <th>05:19</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>1.705812</th>\n",
       "    <th>1.792771</th>\n",
       "    <th>0.630864</th>\n",
       "    <th>05:19</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed 2 new training epochs\n",
      "completed 4 total training epochs\n"
     ]
    }
   ],
   "source": [
    "# if want to continue training existing model, set to True\n",
    "# if want to start fresh from the initialized language model, set to False\n",
    "# also, make sure to remove any previously created saved states before changing\n",
    "# flag back to continue\n",
    "continue_flag = True\n",
    "# Resume interrupted training\n",
    "resume_flag = True\n",
    "########################################################\n",
    "learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)\n",
    "\n",
    "########################################################\n",
    "# set this to how many cycles you want to run\n",
    "num_cycles = 2\n",
    "########################################################\n",
    "lm_base_file = 'mimic_lm_fine_tuned_'\n",
    "if continue_flag:\n",
    "    if os.path.isfile(cycles_file):\n",
    "        with open(cycles_file, 'rb') as f:\n",
    "            prev_cycles = pickle.load(f)\n",
    "        print('This model has been trained for', prev_cycles, 'epochs already')  \n",
    "else:\n",
    "    prev_cycles = 0\n",
    "\n",
    "file = lm_base_file + str(prev_cycles)\n",
    "learner_file = base_path/file\n",
    "callback_save_file = str(learner_file) + '_auto'\n",
    "fn_pattern = callback_save_file + '*'\n",
    "\n",
    "\n",
    "# for one cycle learning with learning rate annealing - where to resume from\n",
    "start_epoch = 0\n",
    "\n",
    "if resume_flag:\n",
    "    training_files = glob.glob(str(base_path/fn_pattern))\n",
    "    if len(training_files) > 0:\n",
    "        training_files.sort()\n",
    "        completed_cycles = int(re.split('_|\\.', training_files[-1])[-2])\n",
    "        if completed_cycles < (num_cycles - 1):\n",
    "            # need to load the last file\n",
    "            print('Previous training cycle of', num_cycles, 'did not complete; finished',\n",
    "                  completed_cycles + 1, 'cycles. Loading last save...')\n",
    "            # load just filename, drop extension of .pth as that is automatically appended inside load function\n",
    "            learn.load(os.path.splitext(training_files[-1])[0])\n",
    "            start_epoch = completed_cycles + 1\n",
    "        else:\n",
    "            print('Previous training cycle of', num_cycles, 'completed fully.')\n",
    "            learn = custom_learner_load(learner_file)\n",
    "    else:\n",
    "        print('No auto save files exist from interupted training.')\n",
    "        if continue_flag:\n",
    "            learn = custom_learner_load(learner_file)\n",
    "        else:\n",
    "            print('Starting training with base language model')\n",
    "else:\n",
    "    if continue_flag:\n",
    "        learn = custom_learner_load(learner_file)\n",
    "    else:\n",
    "        print('Starting training with base language model')\n",
    "    # remove any auto saves\n",
    "    training_files = glob.glob(str(base_path/fn_pattern))\n",
    "    if len(training_files) > 0:\n",
    "        for f in training_files:\n",
    "            print('Deleting', f)\n",
    "            os.remove(f)\n",
    "\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(num_cycles, 5e-3, moms=(0.8,0.7),\n",
    "                    callbacks=[\n",
    "                        callbacks.SaveModelCallback(learn, every='epoch', monitor='accuracy', name=callback_save_file),\n",
    "                        # CSVLogger only logs when num_cycles are complete\n",
    "                        callbacks.CSVLogger(learn, filename='mimic_lm_fine_tune_history', append=True),\n",
    "                        callbacks.EarlyStoppingCallback(learn, monitor='accuracy', min_delta=0.005, patience=5)\n",
    "                    ],\n",
    "                    start_epoch=start_epoch)\n",
    "file = lm_base_file + str(prev_cycles + num_cycles)\n",
    "learner_file = base_path/file\n",
    "learn.save(learner_file)\n",
    "\n",
    "with open(cycles_file, 'wb') as f:\n",
    "    pickle.dump(num_cycles + prev_cycles, f)\n",
    "release_mem()\n",
    "    \n",
    "print('completed', num_cycles, 'new training epochs')\n",
    "print('completed', num_cycles + prev_cycles, 'total training epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/jupyter/mimic/mimic_lm_fine_tuned_4')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = lm_base_file + str(prev_cycles + num_cycles)\n",
    "learner_file = base_path/file\n",
    "learner_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "    Total time: 18:06\n",
    "\n",
    "    epoch \ttrain_loss \tvalid_loss \taccuracy \ttime\n",
    "        0 \t2.915443 \t2.784941 \t0.499640 \t04:30\n",
    "        1 \t2.802181 \t2.660058 \t0.511305 \t04:31\n",
    "        2 \t2.507199 \t2.407333 \t0.537908 \t04:31\n",
    "        3 \t2.360105 \t2.291246 \t0.552453 \t04:31\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this block of code to compare how well a few different learning rates work\n",
    "\n",
    "Found that `5e-3` works best with `learn.unfreeze()`\n",
    "\n",
    "```python\n",
    "num_cycles = 4\n",
    "prev_cycles = 4\n",
    "\n",
    "for lr in [1e-3, 5e-3, 1e-2, 5e-2, 1e-1]:\n",
    "    print('now testing with multiple epochs and learning rate of', lr)\n",
    "    print('This model has been trained for', prev_cycles, 'epochs already')    \n",
    "    file = lm_base_file + str(prev_cycles)\n",
    "    learner_file = base_path/file\n",
    "    learn.load(learner_file)\n",
    "    learn.unfreeze()\n",
    "    print('loaded existing learner from', str(learner_file))\n",
    "\n",
    "\n",
    "    learn.fit_one_cycle(num_cycles, lr, moms=(0.8,0.7))\n",
    "    file = lm_base_file + str(prev_cycles + num_cycles + 1)\n",
    "    learner_file = base_path/file\n",
    "    learn.save(learner_file)\n",
    "    release_mem()\n",
    "\n",
    "    print('completed', num_cycles, 'new training epochs')\n",
    "    print('completed', num_cycles + prev_cycles, 'total training epochs')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For confirmation, she underwent CTA of the lung which was negative for pulmonary embolism \n",
      "  but showed no PE , but did show some pulmonary edema . She has had \n",
      "  some mild dyspnea on exertion but has improved . She was brought to the \n",
      "  ED for further evaluation\n",
      "For confirmation, she underwent CTA of the lung which was negative for pulmonary embolism or dissection . \n",
      "  She was extubated today and given 1 unit of prbcs for Hct of 24 . She is \n",
      "  afebrile , HR in the 120s , BP stable . She is\n"
     ]
    }
   ],
   "source": [
    "# test the language generation capabilities of this model (not the point, but is interesting)\n",
    "TEXT = \"For confirmation, she underwent CTA of the lung which was negative for pulmonary embolism\"\n",
    "N_WORDS = 40\n",
    "N_SENTENCES = 2\n",
    "print(\"\\n\".join(learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder(enc_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the encoder:\n",
    "\n",
    "```python\n",
    "learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)\n",
    "learn.load_encoder(enc_file)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
