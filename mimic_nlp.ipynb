{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using FAST.AI for NLP\n",
    "\n",
    "Exploring the MIMIC III data set medical notes.\n",
    "\n",
    "Tried working with the full dataset, but almost every training step takes many hours (~13 for initial training), predicted 14+ per epoch for fine tuning.\n",
    "\n",
    "Instead will try to work with just 10% sample... Not sure that will work though\n",
    "\n",
    "A few notes:\n",
    "* See https://docs.fast.ai/text.transform.html#Tokenizer for details on what various artificial tokens (e.g xxup, xxmaj, etc.) mean\n",
    "* To view nicely formatted documentation on the fastai library, run commands like: ` doc(learn.lr_find)`\n",
    "\n",
    "### To Do:\n",
    "* need to evalate how changing the learning rate would alter training time\n",
    "* need to evalate how changing the learning rate would alter accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to verify that Torch can find and use your GPU, run the following code:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These next cells can be used to get an idea of the speed up provided by a GPU for some operations (from https://course.fast.ai/gpu_tutorial.html)\n",
    "```python\n",
    "import torch\n",
    "t_cpu = torch.rand(500,500,500)\n",
    "%timeit t_cpu @ t_cpu\n",
    "# separate cell \n",
    "t_gpu = torch.rand(500,500,500).cuda()\n",
    "%timeit t_gpu @ t_gpu\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original data set too large to work with in reasonable time due to limted GPU resources\n",
    "pct_data_sample = 0.01\n",
    "# how much to hold out for validation\n",
    "valid_pct = 0.1\n",
    "\n",
    "# pandas doesn't understand ~, so provide full path\n",
    "base_path = Path('/home/seth/mimic')\n",
    "\n",
    "# files used during processing - all aggregated here\n",
    "notes_file = base_path/'noteevents.pickle'\n",
    "lm_file = 'mimic_lm.pickle' # actual file is at base_path/lm_file but due to fastai function, have to pass file name separately\n",
    "init_model_file = base_path/'mimic_fit_head'\n",
    "cycles_file = base_path/'num_iterations.pickle'\n",
    "lm_base_file = 'mimic_lm_fine_tuned_'\n",
    "enc_file = 'mimic_fine_tuned_enc'\n",
    "class_file = 'mimic_cl.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if this doesn't free memory, can restart Python kernel.\n",
    "# if that still doesn't work, try OS items mentioned here: https://docs.fast.ai/dev/gpu.html\n",
    "def release_mem():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this to see what has already been imported\n",
    "#whos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Random Number seed for repeatability; set Batch Size to control GPU memory\n",
    "\n",
    "See **\"Performance notes\"** section below for how setting batch size impacts GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "# previously used 48; worked fine but never seemed to use even half of GPU memory; 64 still on the small side\n",
    "bs=48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While parsing a CSV and converting to a dataframe is pretty fast, loading a pickle file is much faster.\n",
    "\n",
    "For load time and size comparison:\n",
    "* `NOTEEVENTS.csv` is ~ 3.8GB in size\n",
    "  ```\n",
    "  CPU times: user 51.2 s, sys: 17.6 s, total: 1min 8s\n",
    "  Wall time: 1min 47s\n",
    "  ```\n",
    "* `noteevents.pickle` is ~ 3.7 GB in size\n",
    "  ```\n",
    "  CPU times: user 2.28 s, sys: 3.98 s, total: 6.26 s\n",
    "  Wall time: 6.26 s\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading noteevnt pickle file\n",
      "CPU times: user 6.49 s, sys: 8.77 s, total: 15.3 s\n",
      "Wall time: 2min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "orig_df = pd.DataFrame()\n",
    "if os.path.isfile(notes_file):\n",
    "    print('Loading noteevnt pickle file')\n",
    "    orig_df = pd.read_pickle(notes_file)\n",
    "else:\n",
    "    print('Could not find noteevent pickle file; creating it')\n",
    "    # run this the first time to covert CSV to Pickle file\n",
    "    orig_df = pd.read_csv(base_path/'NOTEEVENTS.csv', low_memory=False, memory_map=True)\n",
    "    orig_df.to_pickle(notes_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to data set size and performance reasons, working with a 10% sample. Use same random see to get same results from subsequent runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = orig_df.sample(frac=pct_data_sample, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to free up some memory\n",
    "# orig_df = None\n",
    "# del orig_df\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df: 93 MB\n"
     ]
    }
   ],
   "source": [
    "from pympler import asizeof\n",
    "print('df:', int(asizeof.asizeof(df) / 1024 / 1024), 'MB')\n",
    "#print('orig_df:', asizeof.asizeof(orig_df))\n",
    "#print('data_lm:', asizeof.asizeof(data_lm, detail=1))\n",
    "#print asizeof.asized(obj, detail=1).format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>CHARTTIME</th>\n",
       "      <th>STORETIME</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>CGID</th>\n",
       "      <th>ISERROR</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1292716</th>\n",
       "      <td>1295263</td>\n",
       "      <td>2549</td>\n",
       "      <td>159440.0</td>\n",
       "      <td>2132-04-02</td>\n",
       "      <td>2132-04-02 13:09:00</td>\n",
       "      <td>2132-04-02 13:35:00</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>18566.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CCU NSG TRANSFER SUMMARY UPDATE: RESP FAILURE\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160271</th>\n",
       "      <td>1175599</td>\n",
       "      <td>29621</td>\n",
       "      <td>190624.0</td>\n",
       "      <td>2149-02-23</td>\n",
       "      <td>2149-02-23 03:27:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Radiology</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[**2149-2-23**] 3:27 AM\\n CHEST (PORTABLE AP) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1549380</th>\n",
       "      <td>1555118</td>\n",
       "      <td>22384</td>\n",
       "      <td>142591.0</td>\n",
       "      <td>2185-03-26</td>\n",
       "      <td>2185-03-26 17:58:00</td>\n",
       "      <td>2185-03-26 18:01:00</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>16985.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Respiratory Care\\nPt remains intubated (#7.5 E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7474</th>\n",
       "      <td>5743</td>\n",
       "      <td>690</td>\n",
       "      <td>152820.0</td>\n",
       "      <td>2182-09-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2182-9-12**]       Dischar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014768</th>\n",
       "      <td>2023163</td>\n",
       "      <td>25560</td>\n",
       "      <td>156143.0</td>\n",
       "      <td>2154-11-18</td>\n",
       "      <td>2154-11-18 10:44:00</td>\n",
       "      <td>2154-11-18 17:08:00</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>16888.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Neonatology\\nOn exam pink active non-dysmorphi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ROW_ID  SUBJECT_ID   HADM_ID   CHARTDATE            CHARTTIME  \\\n",
       "1292716  1295263        2549  159440.0  2132-04-02  2132-04-02 13:09:00   \n",
       "1160271  1175599       29621  190624.0  2149-02-23  2149-02-23 03:27:00   \n",
       "1549380  1555118       22384  142591.0  2185-03-26  2185-03-26 17:58:00   \n",
       "7474        5743         690  152820.0  2182-09-14                  NaN   \n",
       "2014768  2023163       25560  156143.0  2154-11-18  2154-11-18 10:44:00   \n",
       "\n",
       "                   STORETIME           CATEGORY          DESCRIPTION     CGID  \\\n",
       "1292716  2132-04-02 13:35:00      Nursing/other               Report  18566.0   \n",
       "1160271                  NaN          Radiology  CHEST (PORTABLE AP)      NaN   \n",
       "1549380  2185-03-26 18:01:00      Nursing/other               Report  16985.0   \n",
       "7474                     NaN  Discharge summary               Report      NaN   \n",
       "2014768  2154-11-18 17:08:00      Nursing/other               Report  16888.0   \n",
       "\n",
       "         ISERROR                                               TEXT  \n",
       "1292716      NaN  CCU NSG TRANSFER SUMMARY UPDATE: RESP FAILURE\\...  \n",
       "1160271      NaN  [**2149-2-23**] 3:27 AM\\n CHEST (PORTABLE AP) ...  \n",
       "1549380      NaN  Respiratory Care\\nPt remains intubated (#7.5 E...  \n",
       "7474         NaN  Admission Date:  [**2182-9-12**]       Dischar...  \n",
       "2014768      NaN  Neonatology\\nOn exam pink active non-dysmorphi...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ROW_ID           int64\n",
       "SUBJECT_ID       int64\n",
       "HADM_ID        float64\n",
       "CHARTDATE       object\n",
       "CHARTTIME       object\n",
       "STORETIME       object\n",
       "CATEGORY        object\n",
       "DESCRIPTION     object\n",
       "CGID           float64\n",
       "ISERROR        float64\n",
       "TEXT            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20832, 11)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train and test sets; using same random seed so subsequent runs will generate same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 1./3\n",
    "train, test = train_test_split(df, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13888, 11)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6944, 11)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to build initial version of language model; If running with full dataset, requires a **LOT** of RAM; using a **LOT** of CPU helps it to happen quickly as well\n",
    "\n",
    "Questions:\n",
    "\n",
    "* why does this only seem to use CPU? (applies to both both textclasdatabunch and textlist)\n",
    "* for 100% of the mimic noteevents data:\n",
    "  * run out of memory at 32 GB, error at 52 GB, trying 72GB now... got down to only 440MB free; if crash again, increase memory\n",
    "  * now at 20vCPU and 128GB RAM; ok up to 93%; got down to 22GB available\n",
    "  * succeeded with 20CPU and 128GB RAM...\n",
    "* try smaller batch size? will that reduce memory requirements?\n",
    "* with 10% dataset sample, it seems I could get by with perhaps 32GB system RAM\n",
    "\n",
    "For comparison:\n",
    "* 10% langauge model is ~ 1.2 GB in size\n",
    "  * Time to load existing language model:\n",
    "    ```\n",
    "    CPU times: user 3.29 s, sys: 844 ms, total: 4.14 s\n",
    "    Wall time: 12.6 s\n",
    "    ```\n",
    "  * Time to build language model:\n",
    "    ```\n",
    "    CPU times: user 36.9 s, sys: 8.56 s, total: 45.4 s\n",
    "    Wall time: 3min 27s\n",
    "    ```\n",
    "* 100% language model is...\n",
    "  * Time to load existing language model:\n",
    "  * Time to build language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading existing langauge model\n",
      "CPU times: user 1.04 s, sys: 307 ms, total: 1.35 s\n",
      "Wall time: 4.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tmpfile = base_path/lm_file\n",
    "\n",
    "if os.path.isfile(tmpfile):\n",
    "    print('loading existing langauge model')\n",
    "    data_lm = load_data(base_path, lm_file, bs=bs)\n",
    "else:\n",
    "    print('creating new language model')\n",
    "    data_lm = (TextList.from_df(df, 'texts.csv', cols='TEXT')\n",
    "               #df has several columns; actual text is in column TEXT\n",
    "               .split_by_rand_pct(valid_pct=valid_pct, seed=seed)\n",
    "               #We randomly split and keep 10% for validation\n",
    "               .label_for_lm()\n",
    "               #We want to do a language model so we label accordingly\n",
    "               .databunch(bs=bs))\n",
    "    data_lm.save(tmpfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If need to view more data, run appropriate line to make display wider/show more columns...\n",
    "```python\n",
    "# default 20\n",
    "pd.get_option('display.max_columns')\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.max_columns', None) # show all\n",
    "# default 50\n",
    "pd.get_option('display.max_colwidth')\n",
    "pd.set_option('display.max_colwidth', -1) # show all\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>pacs . xxmaj bp went back to 150 / 80 . xxup hr varies between 80s to low 90s at rest up to 1-teens with activity . xxup bp varies more widely between 1-teens / 70s at rest up to 170 / 90s with activity . xxmaj she continues on dilt 90 mg po qid . xxmaj she was xxup k+ replaced today . \\n  xxup resp : xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>inr:17.6 / 35.9 / 1.6 , xxup ck / xxup ckmb / \\n  xxmaj troponin - xxup xxunk / 6 / 0.03 , xxmaj differential - xxmaj neuts:79.9 % , xxmaj lymph:10.8 % , \\n  xxmaj mono:5.8 % , xxmaj eos:3.3 % , xxmaj ca++:8.0 mg / dl , xxmaj mg++:2.0 mg / dl , xxup po4:1.8 mg / dl \\n  xxmaj assessment and xxmaj plan \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[ * * xxmaj last xxmaj name ( namepattern1 ) 1782 * * ] at 11:15 a.m. on [ * * 2181 - 2 - 20 * * ] . \\n \\n  xxbos xxmaj renal failure , acute ( xxmaj acute renal failure , xxup arf ) \\n  xxmaj assessment : \\n  xxmaj pt . anuric s / p nephrectomy . \\n  xxmaj action : \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>xxmaj name ( xxup ni ) 87 * * ] . \\n \\n  p : xxmaj fully awaken and extubate . xxmaj restart cardiac meds as soon as pt able to take po . xxmaj restart oral diabetic meds when pt taking food . xxmaj maintain c - collar at all times , under head pillow when supine . ? transfuse xxup rbc in setting of volume dependent hypotension</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>and suppport as needed . \\n \\n  xxup fen : xxmaj weight 848 g , xxunk . tf=130cc / kg / day . xxmaj enteral feeds \\n  presently at 120cc / kg / day of xxup xxunk 20cc xxmaj q4hrs , xxup pg , gavaged \\n  over 40 minutes and tol well . xxmaj no spits . xxmaj dstick 61 . xxup ivf off \\n  at</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lm.show_batch()\n",
    "# how to look at original version of text\n",
    "#df[df['TEXT'].str.contains('being paralyzed were discussed', case=False)].TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as of June 2019, this automatically loads and initializes the model based on WT103 from\n",
    "# https://s3.amazonaws.com/fast-ai-modelzoo/wt103-fwd.tgz; will auto download if not already on disk\n",
    "learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Learning rate graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEGCAYAAABPdROvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3deXyU5b338c8vO4QsLCGETfZVASGCKyq2uNal1Vatx7pSWrWe9uly+vj0tLU9rT22p5u1Fq1YW5fTaq1SN9wRK0tiQSIiYtjXsIYQyPp7/pihxpgNZu5MZub7fr3mlZn7vuae38UEvtzbdZm7IyIicrRSYl2AiIjENwWJiIhEREEiIiIRUZCIiEhEFCQiIhKRtFgXcKT69OnjQ4YMiXUZIiJxpbS0dKe7FwSx7bgLkiFDhlBSUhLrMkRE4oqZrQ9q2zq0JSIiEVGQiIhIRBQkIiISEQWJiIhEREEiIiIRUZCIiEhEFCQiIhKRuLuPpLNs2lPN/He2U11bT1Z6Kt0yUumWnsqEgXmM6JsT6/JERLoMBUkTO6tqeGbFVp5ctoXS9XtabTflmJ587oRBXDChiO4Z+iMUkeRm8TaxVXFxsQdxZ/tTy7fwjb8sp6a+kdGFOVw4qT+fmtCffnlZHKxr4FBdA/sP1fPyqu08unQj5RUH6JGZxgUTirho0gCmDe1FSopFvS4RkWgws1J3Lw5k28keJO7OL158n1++9D5Th/Ti9ovHM6ZfbrvvKVm/h0eWbOC5sm1U1zZQlJfFhRP7M31UAb2yM+jZPYP87ulkpadGrVYRkaMVt0FiZvnAfcCxgAPXufubTdYb8EvgPKAauMbd32prm9EMkkN1DXzjsbeZt3wLl04ZyH9dciyZaUf2D391bT0vrNzOk8u28NrqChoaP/rn2S83i8uKB3L51MEMyO8WlbpFRI5UPAfJH4DX3f0+M8sAurv73ibrzwNuIRQk04Bfuvu0trYZjSA5VNfAa6sruPuVNSzftI9vnTOG2acPI5RrR29XVQ3vbdvP3oN17KmuZW91HSXrdvPq6goMOGN0Xy6c2J/MtBQa3GlodLIz0jhzTF9SdVhMRAIUZJAEdqbYzHKB6cA1AO5eC9Q2a3YR8KCH0myRmeWbWZG7b412PVU19by8agfPlW3llVUVHKxroGf3dO65ajLnHFsUlc/o3SOTk0dkfmz5pj3VPLpkI48u3cjLq3Z8bP2pI/rwi8sn0afHx98rItLVBbZHYmaTgDnASmAiUArc6u4HmrT5O3CHuy8Mv34J+Ja7lzTb1ixgFsDgwYOnrF9/5KMhP166if/zl+X06ZHJ2eMLOffYIqYN60V6aufdSlPX0Mj726swg9QUI8WMxWt3cfu8leR3T+euKydzwpBenVaPiCSPuDy0ZWbFwCLgFHdfbGa/BCrd/TtN2jwN/LhZkHzT3Utb2+7RHtqqPFTHe9v2M3lwzy53GGnllkq+/FApG/cc5Jtnj+aaU4Yc8bkaEZG2BBkkQf53fBOwyd0Xh18/Bkxuoc2gJq8HAluCKCY3K50ThvTqciECMK5/Lk/dciozxxXy42dXcfztL3DDH0p4aPF6Nu89GOvyRETaFNg5EnffZmYbzWy0u78HnEXoMFdTTwE3m9mjhE627wvi/Eg8yM1K5+7PT+bV1RW8/O4OXl61gxff3Q7AVz8xils/MTLGFYqItCzo27JvAR4KX7FVDlxrZrMB3P0e4BlCV2ytIXT577UB19OlmRlnju7LmaP7crs7H1RUcdfLa/j5i6tJTYGbZyhMRKTrCTRI3H0Z0PyY3D1N1jtwU5A1xCszY0TfHH722UmkmPHT+atJTUnhS2cMj3VpIiIfoYGiurjUFOPOyyZS3+j85LlVpKcaN5w2LNZliYj8i4IkDqSmGP/z2Yk0NDo/fPpddlbV8u+fGBm14Vd2H6jlrfV7KN2wh9L1eyivqGJYnx4cOyCPCQPzGN0vh4N1DezcX0NFVQ0799eyff8hdlTWsCP8s1tGKv1ys+iXF3rkZKWRlmKkpaSQnmqMLcqlWJc2iySkpB9rK57UNTTynb+V8ejSjQwryOYnn5kQ0X0nB2sb+OHTK3lo8QYA0lKM8QPyGFHQg/KdVazcUklNfWOL7+2dnUFBTiaFuVkU5GRysK6BbfsOsW3fIbZXHqK+8eO/V6eN7MPXZ45m4qD8o65ZRI5OXN5HEpRkDpLDFqyu4Nt/XcHmvQe5+qRjuPyEwaSlhm5wTE0xumekkt89vc17Ud7dWslXHvkn7++o4pqTh3DecUVMGJj3kb2c+oZG1lRUsXp7FTmZafTpkUlBTia9e2S0eSNnY6NT29BIfaNT39BIbX0jTy3fwt2vfsDuA7WcPb6Qq048hrxu6XTPSCUrPZXe2Zl0y9C9MyJBUZA0oSAJOVBTz53Pv8cf3lxHa19hj8w08runU5SXxdiiXMb3z2VcUR6l63fzo2dXkdctnf/57EROG1nQKTVX1dRz/8K13LugnP019R9Zl5WewtUnDeGL04fRW0PFiESdgqQJBclHrdpWSXnFARoancbwQJDVtQ3sOVDLnurQ4JEbdlezamslB2ob/vW+GWP68t+XTojJ+F77quso27KPg7UNVNc1cKi2gTfLd/Hkss1kpady9UlDmDV9GL2yMzq9NpFEpSBpQkFydBobnQ27q1m5tZIUg7PH94t4tONo+6Ciil+99D5PLd9Ct/RUrpg6mOtPHUp/Db8vEjEFSRMKksT3/vb9/OaVNcx7eysGXDixP7NOH9buhGMi0joFSRMKkuSxaU819y9cx6NLN1Bd28Dowhxmji9k5rh+HDsgt8vtUYl0ZQqSJhQkyWdvdS2Pv7WZ+e9sY+m63TQ69M/L4pvnjOGiSf0VKCIdoCBpQkGS3HYfqOXlVTv406L1LNu4l/OPK+KHFx9LT52YF2lTvA4jLxJ1vbIzuHTKQB7/0sl865wxzF+5jZm/WMDLq7bHujSRpKUgkbiUmmJ86YzhPHnTqfTOzuC6B0qY9WAJZZv3xbo0kaSjsbYkro3rn8uTN5/C714r577Xy5m/cjszxvTllhkjOH5wz06poa6hkQ27q1m38wAV+2vYdaCWnVU17K2u45je3Zk2tDfHD86P2thoIl2NzpFIwqg8VMeD/1jHfQvXsre6jkG9ujG2Xy5jinIZV5TD5GN60jcnK+LP2Vddx3PvbOWld3ewZkcVG3ZXf2xssZzMNHK7pbNl30HcISM1hUmD8jljTAEXHNefwb27R1yHyJHQyfYmFCTSngM19fy5ZCMl6/fw7tZK1u08QKODGUwZ3JNzju3H2eP7MahXx/4xP1TXwJa9BynbUsm85Vt47b0KahsaGdizG8cNyGNYQTbD+vRgaEE2hblZ9M7O+Nfex77qOpau283itbt4s3wXZZsrAZgwMI/zjytixpi+DC/oQUoXnAJaEouCpAkFiRypg7UNvLd9P6+9V8Fz72zj3a2hf8z79MggLSWFtFQjLcVIT00JPdJSyEg16hqczXsPUrG/5l/b6puTyQUT+nPRpP5MGJh3xJceb9xdzbNlW3n67a0s3xQ6n5OblcakwT2ZPDif00b2YfLgnrqkWaJOQdKEgkQitX7XAZ5/Zxtrd1bT0NhIfYNT3+jUNTRS13D4ZyMpZvTPz2Jgz+4MyO/GkD7ZTBqUT2qU9h427q7mzfJd/HPDHt5av5fVO/bjDsMLsvncCYP49OSBMRkLTRJT3AaJma0D9gMNQH3zTphZT+B+YDhwCLjO3cva2qaCRBLVvoN1PP/ONv536UZK1+8hLcU4+9h+3HzmCMYWaXgYiUy8B0mxu+9sZf2dQJW7f9/MxgC/cfez2tqmgkSSwZod+/nfpRt5dMlG9tfUc874fnzlrJGM669AkaOTyDckjgNeAnD3VcAQMyuMbUkisTeibw63nT+Ohd+awa1njeSNNTs571evM/uPpZRXVMW6PJGPCDpIHJhvZqVmNquF9cuBTwOY2VTgGGBg80ZmNsvMSsyspKKiItCCRbqSvO7pfPWTo1j4rRl85ayRvP5+BTN/voDvPlnG7gO1sS5PBAj+0FZ/d99iZn2BF4Bb3H1Bk/W5wC+B44EVwBjgBndf3to2dWhLklnF/hp+/uJqHl2ygeyMNG45awQ3nDpMlw9Lu+L2HMlHPsjse4TOh/y0lfUGrAUmuHtla9tRkIiE5mz50TPv8sp7FVwwoYiffXYimWm6c15aF5fnSMws28xyDj8HZgJlzdrkm9nhYVtvABa0FSIiEjKyMIe5107l2+eO4e9vb+X6B0qoqqmPdVmSpII8R1IILDSz5cAS4Gl3f87MZpvZ7HCbscA7ZrYKOBe4NcB6RBLOF08fzk8vm8ib5bu4Ys4idlbVtP8mkSjTDYkiCeDlVdv58kNv0S83i99eNUX3ncjHxOWhLRHpPDPGFPLQDSdSVdPAhXct5K6X36e+oTHWZUmSUJCIJIgpx/Rk/lenc/b4fvx0/mo+/dt/8P72/bEuS5KAgkQkgfTKzuCuKyfzmysns3F3Nef/eiH3vV5OY2N8HcKW+KIgEUlA508oYv5XT+e0EX344dPvcs0DS9mx/1Csy5IEpSARSVAFOZnc94VifnDReBaX7+LcX7yuue0lEAoSkQRmZvzbSUOYd8upFORkct0DJfzmlTWxLksSjIJEJAmMKszhbzedwkWT+nPn8+/xwBtrY12SJJC0WBcgIp0jKz2Vn102keraBr43byU9stK5dMrHxkgVOWLaIxFJImmpKfz6iuM5ZURvvvnYcp4r2xrrkiQBKEhEkkxWeipz/q2YiYPy+cojy1iwWlMzSGQUJCJJKDszjQeumcqwgmy++MdSStbtjnVJEscUJCJJKq97On+8fhr98rK4du5Syjbvi3VJEqcUJCJJrCAnkz/dMI2crDSuvn8Ja3ZoGl85cgoSkSQ3IL8bf7phGikGV923mI27q2NdksQZBYmIMKygB3+8fhrVtfVced8ituw9GOuSJI4oSEQEgLFFuTx4/TT2HqjjynsXsW2fxuaSjlGQiMi/TBqUzwPXTaVifw1X3rtIAz1KhwQaJGa2zsxWmNkyM/vYtIZmlmdm88xsuZm9Y2bXBlmPiLRvyjE9mXvtVLbuO8Tn712s6XulXZ2xR3Kmu09qZYrHm4CV7j4ROAP4mZlldEJNItKGqUN7cf81J7BxTzWfv3cxFfsVJtK6WB/aciDHzAzoAewG6mNbkogAnDS8N/d/4QQ27K7mc3Pe1DkTaVXQQeLAfDMrNbNZLay/CxgLbAFWALe6uyaaFukiTh7Rhz9cN5Xt+w7xuTlvsllXc0kLgg6SU9x9MnAucJOZTW+2/mxgGdAfmATcZWa5zTdiZrPMrMTMSioqNC6QSGeaOrQXf7xhGrsP1PLZe95kwy7dZyIfFWiQuPuW8M8dwBPA1GZNrgX+6iFrgLXAmBa2M8fdi929uKCgIMiSRaQFkwf35JEbT+RAbT2f/d2brN91INYlSRcSWJCYWbaZ5Rx+DswEypo12wCcFW5TCIwGyoOqSUSO3rED8njkxhOpqW/gijmLtGci/xLkHkkhsNDMlgNLgKfd/Tkzm21ms8NtfgCcbGYrgJeAb7n7zgBrEpEIjC3K5aEbTqS6roEr7l2k4VQEAHP3WNdwRIqLi72k5GO3pIhIJyrbvI/P37eYnKw0Hp11IgN7do91SdIOMytt5TaMiMX68l8RiUPHDsjjT9dPo/JgHVfcu4gdlbo0OJkpSETkqBw3MI8Hr5/GrqpavjB3KZWH6mJdksSIgkREjtqkQfn89qopvL99P198sJSa+oZYlyQxoCARkYicPqqAOy+bwJvlu/jan5fT2Bhf510lcmmxLkBE4t8lxw+kYn8NP3pmFQU9Mvnup8YRGvlIkoGCRESi4sbThrG9sobfL1xL//wsZk0fHuuSpJMoSEQkKsyM284by7bKQ/zomVUMyO/O+ROKYl2WdAIFiYhETUqK8bPLJrJ93yG++udlFOZmUjykV6zLkoDpZLuIRFVWeir3Xl3MgPxu3PBgCeUVVbEuSQKmIBGRqOuZncED155AqhnXzF3KngO1sS5JAqQgEZFAHNM7m3u/UMyWvQe549lVsS5HAqQgEZHATB7ck+tPHcr/lmykZN3uWJcjAVGQiEigvnLWSPrnZfH//lZGXYMmQE1EChIRCVR2ZhrfvXA8q7bt54E31sW6HAmAgkREAjdzXCFnjenLz19czRbN+55wFCQiEjgz43sXjqfRne/PeyfW5UiUKUhEpFMM6tWdr5w1kuff2c4zK7bGuhyJIgWJiHSaG04dxsRB+Xz9L8t5d2tlrMuRKAk0SMxsnZmtMLNlZvax+XHN7BvhdcvMrMzMGsxM4ymIJKiMtBTm/NsUcrLSuOEPJeyqqol1SRIFnbFHcqa7T2pprmB3vzO8bhLwbeA1d9fF5iIJrDA3izn/VkxFVQ1feugtaut1SXC860qHtq4AHol1ESISvImD8vnvz0xgydrdfG/eO7hrMqx4FnSQODDfzErNbFZrjcysO3AO8Hgr62eZWYmZlVRUVARUqoh0pouPH8Ds04fz8OINPP7W5liXIxEIOkhOcffJwLnATWY2vZV2nwLeaO2wlrvPcfdidy8uKCgIqlYR6WTfOHs0kwfnc+fzqzhYq/ne41WgQeLuW8I/dwBPAFNbaXo5OqwlknRSU4z/OHcs2ytr+MOb62JdjhylwILEzLLNLOfwc2AmUNZCuzzgdODJoGoRka5r6tBenDm6gLtfWcO+6rpYlyNHIcg9kkJgoZktB5YAT7v7c2Y228xmN2l3CTDf3Q8EWIuIdGHfOHsM+2vquWfBB7EuRY5Ch6baNbPhwCZ3rzGzM4AJwIPuvre197h7OTCxheX3NHv9APBAx0sWkUQzrn8uF03sz9w31nLNyUMozM2KdUlyBDq6R/I40GBmI4DfA0OBhwOrSkSSztc+OZr6BudXL70f61LkCHU0SBrdvZ7QYahfuPtXgaLgyhKRZDO4d3eunDaYR5duZO1OHemOJx0NkjozuwL4AvD38LL0YEoSkWR184wRZKSmaK8kznQ0SK4FTgL+y93XmtlQ4E/BlSUiyahvThZXTB3MvOVbNG9JHOlQkLj7Snf/irs/YmY9gRx3vyPg2kQkCV136hAcmPvG2liXIh3UoSAxs1fNLDc8Mu9yYK6Z/U+wpYlIMhrYszvnH1fEI0s2UnlI95XEg44e2spz90rg08Bcd58CfCK4skQkmd142jCqaup5ZPGGWJciHdDRIEkzsyLgs3x4sl1EJBDHDczjpGG9mfvGOg0zHwc6GiS3A88DH7j7UjMbBuiyChEJzKzTh7Gt8hB/f3tLrEuRdnT0ZPtf3H2Cu38p/Lrc3T8TbGkikszOGFXAyL49mLOgXPOVdHEdPdk+0MyeMLMdZrbdzB43s4FBFyciycvMuHH6MFZt28/r7++MdTnSho4e2poLPAX0BwYA88LLREQCc9Gk/vTNyeR3GsyxS+tokBS4+1x3rw8/HgA0w5SIBCozLZXrTh3KG2t28famVseIlRjraJDsNLOrzCw1/LgK2BVkYSIiAJ+fNpicrDTueU17JV1VR4PkOkKX/m4DtgKXEho2RUQkUDlZ6Vx90jE8W7aN8oqqWJcjLejoVVsb3P1Cdy9w977ufjGhmxNFRAJ3zclDyUhNYc6C8liXIi2IZIbEr0WtChGRNhTkZPLZ4kE8/tYmtu07FOtypJlIgsSiVoWISDtmTR9Go8P9Gsyxy4kkSNq9Q8jM1pnZCjNbZmYlrbQ5I7z+HTN7LYJ6RCSBDerVnU9NKOKhRevZV63BHLuSNoPEzPabWWULj/2E7inpiDPdfZK7F7ew/XzgbuBCdx8PXHbEPRCRpDH7jOEcqG3gwTfXxboUaaLNIHH3HHfPbeGR4+5pUfj8K4G/uvuG8OftiMI2RSRBjemXy4wxfbn/jbVU19bHuhwJi+TQVkc4MN/MSs1sVgvrRwE9w/OdlJrZ1S1txMxmmVmJmZVUVFQEWrCIdG03nTmCPdV1PKwh5ruMoIPkFHefDJwL3GRm05utTwOmAOcDZwPfMbNRzTfi7nPcvdjdiwsKdEO9SDKbckxPThrWmzkLyjlU1xDrcoSAg8Tdt4R/7gCeAKY2a7IJeM7dD7j7TmABMDHImkQk/t0yYwQ79tfwWOmmWJciBBgkZpZtZjmHnwMzgbJmzZ4ETjOzNDPrDkwD3g2qJhFJDCcN783xg/P57asfUNegia9iLcg9kkJgoZktB5YAT7v7c2Y228xmA7j7u8BzwNvhNve5e/OwERH5CDPjlhkj2Lz3IH/75+ZYl5P0LN4mjCkuLvaSkhZvSRGRJOLunP+rhRyqa+CFr51OaorukW6LmZW2dBtGNAR9sl1EJBBmxs0zRlC+8wDPrNga63KSmoJEROLWOeP7MaJvD37zyhpNxxtDChIRiVspKcbs04ezatt+Xlute8xiRUEiInHtwon9KczN5N7XNcR8rChIRCSuZaSlcO0poel4yzbvi3U5SUlBIiJx74qpg8nOSOU+7ZXEhIJEROJeXrd0rpg6mHlvb2XL3oOxLifpKEhEJCFce+pQAO5fqImvOpuCREQSwoD8blwwoYhHlmxg30FNfNWZFCQikjBuPG0YB2obeHSJhpjvTAoSEUkYxw7I45QRvZn7xjpq6jXEfGdRkIhIQpl9+nC2VR7SEPOdSEEiIgnl1BF9mDw4n7tf+YDaeg0x3xkUJCKSUMyMWz8xis17D2qvpJMoSEQk4Uwf2YfjB+fzm1fWaK+kEyhIRCThmBn/rr2STqMgEZGENH1kHyYN0l5JZwg0SMxsnZmtMLNlZvaxaQ3N7Awz2xdev8zM/jPIekQkeYT2Skayee9BHn9LeyVBSuuEzzjT3Xe2sf51d7+gE+oQkSRz+qgCJg7K566X1/CZyQPJSNNBmCDoT1VEEpb2SjpH0EHiwHwzKzWzWa20OcnMlpvZs2Y2PuB6RCTJnNFkr0TnSoIRdJCc4u6TgXOBm8xserP1bwHHuPtE4NfA31raiJnNMrMSMyupqNB0miLScU33SnQFVzACDRJ33xL+uQN4ApjabH2lu1eFnz8DpJtZnxa2M8fdi929uKCgIMiSRSQBnTGqQFdwBSiwIDGzbDPLOfwcmAmUNWvTz8ws/HxquJ5dQdUkIslJeyXBCnKPpBBYaGbLgSXA0+7+nJnNNrPZ4TaXAmXhNr8CLnd3D7AmEUlSp2uvJDCBXf7r7uXAxBaW39Pk+V3AXUHVICJy2OG9kmvmLuWx0k1cOW1wrEtKGLr8V0SSxumjCjQGVwAUJCKSNJqOwfV7ze0eNQoSEUkq00f2Yea4Qn7x4mrW7jwQ63ISgoJERJKKmfGDi48lIy2F/3j8bRobdX1PpBQkIpJ0CnOz+L/njWXx2t08unRjrMuJewoSEUlKl58wiBOH9eLHz7zLtn2HYl1OXFOQiEhSMjPu+PQEahsa+c6TZegWtqOnIBGRpDWkTzZf++QoXli5nadXbI11OXFLQSIiSe36U4cyYWAe333yHXZV1cS6nLikIBGRpJaWmsKdl06k8lAd35u3MtblxCUFiYgkvdH9crhlxkjmLd/C8+9si3U5cUdBIiICfOmM4YwryuW2J8rYW10b63LiioJERARIT03hzssmsLe6ltv/rkNcR0JBIiISNr5/Hl8+Yzh/fWszL6/aHuty4oaCRESkiZtnjGR0YQ63PVHG/kN1sS4nLihIRESayEhL4SeXTmB75SHueHZVrMuJCwoSEZFmJg3K57pThvLQ4g0sKtfs3+1RkIiItOD/zBzN4F7d+Y/H3+ZQXUOsy+nSAg0SM1tnZivMbJmZlbTR7gQzazCzS4OsR0Sko7plpHLHZ45j3a5qfv7i6liX06V1xh7Jme4+yd2LW1ppZqnAT4DnO6EWEZEOO3l4H66YOoh7F5Tz9qa9sS6ny+oKh7ZuAR4HdsS6EBGR5r593lgKcjL59l9X0KBJsFoUdJA4MN/MSs1sVvOVZjYAuAS4p62NmNksMysxs5KKioqAShUR+bjcrHS+c8E43tlSyZ8WrY91OV1S0EFyirtPBs4FbjKz6c3W/wL4lru3eSbL3ee4e7G7FxcUFARVq4hIi84/rojTRvbhp/PfY8d+TYLVXKBB4u5bwj93AE8AU5s1KQYeNbN1wKXA3WZ2cZA1iYgcKTPj+xeOp6aukR8/o3tLmgssSMws28xyDj8HZgJlTdu4+1B3H+LuQ4DHgC+7+9+CqklE5GgNK+jBrOnDeOKfm3VvSTNB7pEUAgvNbDmwBHja3Z8zs9lmNjvAzxURCcRNZ45gYM9ufOdvZdQ1NMa6nC4jLagNu3s5MLGF5S2eWHf3a4KqRUQkGrplpPK9T43nhgdL+P3Ctcw+fXisS+oSusLlvyIiceMT4wr5xNhCfvni+2zcXR3rcroEBYmIyBG6/aLxpBj8v7+V4a57SxQkIiJHqH9+N75+9mheW13BU8u3xLqcmFOQiIgchatPGsLEQfncPm8lew4k99S8ChIRkaOQmmLc8enj2Hewjh89826sy4kpBYmIyFEaW5TLjdOH8ZfSTfxjzc5YlxMzChIRkQjcetZIjundnf/7xAoO1ibnvCUKEhGRCGSlp/LjTyf3vCUKEhGRCIXmLRnMfa+Xs2xj8s1boiAREYmCb583hsLcLL752HJq6pPrEJeCREQkCnKz0vnRJcexensVv3l5TazL6VQKEhGRKDlzTF8+ffwA7n71A1ZuqYx1OZ1GQSIiEkX/+alx5HfP4Ot/WZ40V3EpSEREoii/ewY/+cxxvLutkq/9eRmNSTDPu4JERCTKzhpbyG3njeXZsm385PnEn1ExsPlIRESS2fWnDmX9rmp+91o5x/TK5sppg2NdUmAUJCIiATAzvvupcWzaU813nixjQM9unD6qINZlBUKHtkREApKWmsKvr5zMqMIcbnrorYS9WTHQIDGzdWa2wsyWmVlJC+svMrO3D683s1ODrEdEpLP1yExj7jUn0Cs7g6t/v5iyzftiXVLUdcYeyZnuPsndi1tY9xIw0d0nAdcB93VCPSIinapfXhYP3ziNnCp71hUAAAo1SURBVKx0rvr94oS7xySmh7bcvco/nKcyG0j86+REJCkN7NmdR248kW7pqVz1+8Ws3r4/1iVFTdBB4sB8Mys1s1ktNTCzS8xsFfA0ob2SltrMCh/6KqmoqAiwXBGR4Azu3Z2HbzyRtBTjynsX8/amxDhnEnSQnOLuk4FzgZvMbHrzBu7+hLuPAS4GftDSRtx9jrsXu3txQUFiXvUgIslhaJ9sHr7xRDLTUrjsnjd5ctnmWJcUsUCDxN23hH/uAJ4AprbRdgEw3Mz6BFmTiEisjejbg6duPoWJg/K59dFl/PjZd2lo5w74fdV17Kqq6aQKj0xgQWJm2WaWc/g5MBMoa9ZmhJlZ+PlkIAPYFVRNIiJdRe8emfzp+mlcdeJgfvdaOdc+sJRnV2xl1bZKDtWFxujauLua+xeu5Yo5i5j8wxe4/421Ma66ZUHekFgIPBHOiTTgYXd/zsxmA7j7PcBngKvNrA44CHyuycl3EZGElpGWwg8vPo6xRbl8/6mVLFgdOgdsBr2zM9kZ3gMZVdiDL04fxvnH9Y9lua2yePt3u7i42EtKPnZLiohIXDtQU8/anQco33mA8ooqNu4+yNiiHD45rpBjemdHvH0zK23lNoyIaYgUEZEuIDszjWMH5HHsgLxYl3LENESKiIhEREEiIiIRUZCIiEhEFCQiIhIRBYmIiEREQSIiIhFRkIiISEQUJCIiEpG4u7PdzCqA9c0W5wHNpx1rvqyt14efN13WB9h5lGW2VM+RtDnS/rT3PJK+tFdre20S6bvpSF+aLwvyu9HvWdvL4/X3rLV1kX432e4ezPDp7h73D2BOe8vaen34ebNlJdGs50jaHGl/2nseSV8i7U8ifTcd6Utnfjf6PUvM37Ou+N2090iUQ1vzOrCsrdfzWmkTzXqOpM2R9qcjzyMRSX8S6bvpSF+aLwvyu9HvWdvL4/X3rLV1sfxu2hR3h7Y6i5mVeEADnHW2ROoLJFZ/1JeuK5H6E3RfEmWPJAhzYl1AFCVSXyCx+qO+dF2J1J9A+6I9EhERiYj2SEREJCIKEhERiUjCB4mZ3W9mO8ysrP3WH3vvFDNbYWZrzOxXh+eXD6+7xczeM7N3zOy/o1t1mzVFvT9m9j0z22xmy8KP86JfeYv1BPLdhNd/3czczPpEr+J2awriu/mBmb0d/l7mm1mnzLUaUF/uNLNV4f48YWb50a+81ZqC6M9l4b//jWYW+En5SPrQyva+YGbvhx9faLK8zb9bLQry2uKu8ACmA5OBsqN47xLgJMCAZ4Fzw8vPBF4EMsOv+8Z5f74HfD0RvpvwukHA84RuXO0Tz/0Bcpu0+QpwTxz3ZSaQFn7+E+Ancf7djAVGA68CxV21D+H6hjRb1gsoD//sGX7es63+tvVI+D0Sd18A7G66zMyGm9lzZlZqZq+b2Zjm7zOzIkJ/id/00J/ug8DF4dVfAu5w95rwZ+wIthcfCqg/MRFgX34OfBPo1CtJguiPu1c2aZpNJ/UpoL7Md/f6cNNFwMBge/GhgPrzrru/1xn1hz/vqPrQirOBF9x9t7vvAV4AzjnafycSPkhaMQe4xd2nAF8H7m6hzQBgU5PXm8LLAEYBp5nZYjN7zcxOCLTa9kXaH4Cbw4cc7jeznsGV2q6I+mJmFwKb3X150IV2UMTfjZn9l5ltBD4P/GeAtbYnGr9nh11H6H+7sRTN/sRKR/rQkgHAxiavD/frqPqb1sEPTRhm1gM4GfhLk0N/mS01bWHZ4f8NphHaHTwROAH4s5kNCyd4p4pSf34L/CD8+gfAzwj9Re9UkfbFzLoDtxE6hBJzUfpucPfbgNvM7NvAzcB3o1xqu6LVl/C2bgPqgYeiWeORiGZ/YqWtPpjZtcCt4WUjgGfMrBZY6+6X0Hq/jqq/SRckhPbC9rr7pKYLzSwVKA2/fIrQP65Nd70HAlvCzzcBfw0HxxIzayQ0KFpFkIW3IuL+uPv2Ju+7F/h7kAW3IdK+DAeGAsvDf7EGAm+Z2VR33xZw7S2Jxu9aUw8DTxODICFKfQmf1L0AOCsW//FqItrfTSy02AcAd58LzAUws1eBa9x9XZMmm4AzmrweSOhcyiaOpr9BnyDqCg9gCE1OUAH/AC4LPzdgYivvW0por+PwSafzwstnA7eHn48itItocdyfoiZtvgo8Gq99adZmHZ14sj2g72Zkkza3AI/FcV/OAVYCBZ35nQT9u0YnnWw/2j7Q+sn2tYSOrPQMP+/Vkf62WFcsvtBO/uV5BNgK1BFK2+sJ/a/1OWB5+Bf7P1t5bzFQBnwA3MWHIwFkAH8Kr3sLmBHn/fkjsAJ4m9D/woritS/N2qyjc6/aCuK7eTy8/G1CA/ANiOO+rCH0n65l4UenXIEWYH8uCW+rBtgOPN8V+0ALQRJefl34O1kDXNtef9t6aIgUERGJSLJetSUiIlGiIBERkYgoSEREJCIKEhERiYiCREREIqIgkYRgZlWd/Hn3mdm4KG2rwUKj+5aZ2bz2RsU1s3wz+3I0PlskGnT5ryQEM6ty9x5R3F6afzjAYKCa1m5mfwBWu/t/tdF+CPB3dz+2M+oTaY/2SCRhmVmBmT1uZkvDj1PCy6ea2T/M7J/hn6PDy68xs7+Y2TxgvpmdYWavmtljFppH46HDczOElxeHn1eFB1ZcbmaLzKwwvHx4+PVSM7u9g3tNb/LhAJQ9zOwlM3vLQvNDXBRucwcwPLwXc2e47TfCn/O2mX0/in+MIu1SkEgi+yXwc3c/AfgMcF94+SpgursfT2g03R81ec9JwBfcfUb49fHAvwPjgGHAKS18TjawyN0nAguAG5t8/i/Dn9/ueEXhcZ7OIjS6AMAh4BJ3n0xoDpyfhYPsP4AP3H2Su3/DzGYCI4GpwCRgiplNb+/zRKIlGQdtlOTxCWBck5FRc80sB8gD/mBmIwmNbJre5D0vuHvTOR+WuPsmADNbRmiso4XNPqeWDwe6LAU+GX5+Eh/O5fAw8NNW6uzWZNulhOaGgNBYRz8Kh0IjoT2VwhbePzP8+Gf4dQ9CwbKglc8TiSoFiSSyFOAkdz/YdKGZ/Rp4xd0vCZ9veLXJ6gPNtlHT5HkDLf+dqfMPTza21qYtB919kpnlEQqkm4BfEZp/pACY4u51ZrYOyGrh/Qb82N1/d4SfKxIVOrQliWw+ofk7ADCzw8Nt5wGbw8+vCfDzFxE6pAZweXuN3X0foel0v25m6YTq3BEOkTOBY8JN9wM5Td76PHBdeH4KzGyAmfWNUh9E2qUgkUTR3cw2NXl8jdA/ysXhE9ArCQ3/D/DfwI/N7A0gNcCa/h34mpktAYqAfe29wd3/SWgk18sJTfxUbGYlhPZOVoXb7ALeCF8ufKe7zyd06OxNM1sBPMZHg0YkULr8VyQg4RkbD7q7m9nlwBXuflF77xOJNzpHIhKcKcBd4Sut9hKD6YtFOoP2SEREJCI6RyIiIhFRkIiISEQUJCIiEhEFiYiIRERBIiIiEfn/R/ND+ZMs43AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot(skip_end=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial model training\n",
    "\n",
    "Time to run:\n",
    "\n",
    "* Full data set took about 13 hours using the Nvidia P1000\n",
    "* Full data set was predicted to take about 25 hours with the T4\n",
    "* 10% data took about 1 hour (1:08) using the Nvidia P1000\n",
    "* 10% data is predicted to take about 2.5 hour (actual 2:42) using the Nvidia GTX 1060\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded learner\n"
     ]
    }
   ],
   "source": [
    "# no idea how long nor how much resources this will take\n",
    "# not sure 1e-2 is the right learning rate; maybe 1e-1 or between 1e-2 and 1e-1\n",
    "# using t4\n",
    "# progress bar says this will take around 24 hours... ran for about 52 minutes\n",
    "# gpustat/nvidia-smi indicates currently only using about 5GB of GPU RAM\n",
    "# using p100\n",
    "# progress bar says this will take around 12 hours; took 13:16\n",
    "# at start GPU using about 5GB RAM\n",
    "# after about 8 hours GPU using about 7.5GB RAM.\n",
    "# looks like I could increase batch size...\n",
    "# with bs=64, still only seems to be using about 7GB GPU RAM after running for 15 minutes. \n",
    "# will check after a bit, but likely can increase batch size further\n",
    "\n",
    "if os.path.isfile(str(init_model_file) + '.pth'):\n",
    "    learn.load(init_model_file)\n",
    "    print('loaded learner')\n",
    "else:\n",
    "    learn.fit_one_cycle(1, 5e-2, moms=(0.8,0.7))\n",
    "    learn.save(init_model_file)\n",
    "    print('generated new learner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "continue from initial training - reload in case just want to continue processing from here.\n",
    "\n",
    "As an FYI pytorch automatically appends .pth to the filename, you cannot provide it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)\n",
    "#learn.load(init_model_file)\n",
    "#print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj resp xxmaj care xxmaj note : \\n \\n  xxmaj pt cont trached on cool mist aerosol as</td>\n",
       "      <td>per xxmaj carevue . xxmaj lung sounds coarse dim @ bases suct xxunk th pale yellow sput . xxmaj pt</td>\n",
       "      <td>. well carevue carevue . xxmaj pt sounds clear suct in bases . sm sm . yellow . . xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>: \\n  xxmaj swan - xxmaj ganz catheter with tip at the periphery of the central pulmonary artery .</td>\n",
       "      <td>\\n  xxmaj new right internal jugular venous catheter with the tip in the lower superior \\n  vena cava</td>\n",
       "      <td>xxmaj xxmaj the xxup subclavian jugular line catheter tip its tip in the right \\n  vena vena cava</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>. \\n  xxup issues : \\n  - xxup tbm , s / p xxunk and posterior tracheal splinting</td>\n",
       "      <td>via r \\n  thoracotomy [ * * 1 - 23 * * ] \\n  - subcutaneous emphysema \\n</td>\n",
       "      <td>, xxup xxup arm . * * xxmaj - 23 * * ] . xxup xxmaj xxup . xxup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\\n  30 \\n  31 \\n  xxmaj glucose \\n  156 \\n  94 \\n  75 \\n</td>\n",
       "      <td>110 \\n  85 \\n  xxmaj other labs : xxup pt / xxup ptt / xxup inr:13.6 / 32.1</td>\n",
       "      <td>xxmaj \\n  xxmaj other labs : xxup pt / xxup ptt / xxup xxunk / 1.2 / 1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxmaj midazolam ( xxmaj versed ) - [ * * 2111 - 12 - 7 * * ] 05:30 xxup</td>\n",
       "      <td>am \\n  xxmaj fentanyl - [ * * 2111 - 12 - 7 * * ] 05:30 xxup am</td>\n",
       "      <td>am \\n  xxmaj other - [ * * 2125 - 4 - 26 * * ] 10:00 xxup am</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model has been trained for 8 epochs already\n"
     ]
    }
   ],
   "source": [
    "prev_cycles = 0\n",
    "\n",
    "if os.path.isfile(cycles_file):\n",
    "    with open(cycles_file, 'rb') as f:\n",
    "        prev_cycles = pickle.load(f)\n",
    "print('This model has been trained for', prev_cycles, 'epochs already')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Now fine tune language model\n",
    "\n",
    "Performance notes w/P100 GPU:\n",
    "\n",
    "* at batch size of 128 takes about 1:14:00 per epoch; GPU usage is about 14GB; RAM usage is about 10GB\n",
    "* at batch size of 96 takes about 1:17:00 per epoch; GPU usage is about  9GB; RAM usage is about 10GB\n",
    "* at batch size of 48 takes about 1:30:00 per epoch; GPU usage is about  5GB; RAM usage is about 10GB\n",
    "\n",
    "With `learn.fit_one_cycle(8, 5e-3, moms=(0.8,0.7))` (8 cycles)\n",
    "* gets from about 62.7% accuracy to 67.6% accuracy\n",
    "* Total time: 9:54:16\n",
    "\n",
    "\n",
    "    epoch \ttrain_loss \tvalid_loss \taccuracy \ttime\n",
    "    0 \t1.926960 \t1.832659 \t0.627496 \t1:14:14\n",
    "    1 \t1.808083 \t1.755725 \t0.637424 \t1:14:15\n",
    "    2 \t1.747903 \t1.697741 \t0.645431 \t1:14:15\n",
    "    3 \t1.714081 \t1.652703 \t0.652703 \t1:14:19\n",
    "    4 \t1.637801 \t1.602961 \t0.660170 \t1:14:15\n",
    "    5 \t1.596906 \t1.553225 \t0.668557 \t1:14:14\n",
    "    6 \t1.572020 \t1.519172 \t0.674477 \t1:14:26\n",
    "    7 \t1.517364 \t1.510010 \t0.676342 \t1:14:14\n",
    "\n",
    "\n",
    "Output from first 3 runs:\n",
    "\n",
    "    epoch \ttrain_loss \tvalid_loss \taccuracy \ttime\n",
    "        0 \t1.828720 \t1.741310 \t0.646276 \t3:03:05\n",
    "\n",
    "     1 addtional run of fit_one_cycle complete\n",
    "\n",
    "    epoch \ttrain_loss \tvalid_loss \taccuracy \ttime\n",
    "        0 \t1.736914 \t1.701096 \t0.652299 \t3:03:00\n",
    "\n",
    "     2 addtional run of fit_one_cycle complete\n",
    "\n",
    "    epoch \ttrain_loss \tvalid_loss \taccuracy \ttime\n",
    "        0 \t1.699437 \t1.677218 \t0.655742 \t3:03:02\n",
    "\n",
    "     3 addtional run of fit_one_cycle complete\n",
    "    completed 3 new training epochs\n",
    "    completed 3 total training epochs\n",
    "    \n",
    "Output from next 4 runs:\n",
    "\n",
    "    loaded existing learner from /home/seth/mimic/mimic_lm_fine_tuned_3\n",
    "\n",
    "    epoch \ttrain_loss \tvalid_loss \taccuracy \ttime\n",
    "        0 \t1.693833 \t1.664847 \t0.658170 \t3:03:05\n",
    "\n",
    "         1 addtional run of fit_one_cycle complete\n",
    "\n",
    "    epoch \ttrain_loss \tvalid_loss \taccuracy \ttime\n",
    "        0 \t1.745765 \t1.653829 \t0.659691 \t3:02:57\n",
    "\n",
    "         2 addtional run of fit_one_cycle complete\n",
    "\n",
    "    epoch \ttrain_loss \tvalid_loss \taccuracy \ttime\n",
    "        0 \t1.741647 \t1.648660 \t0.660596 \t3:02:53\n",
    "\n",
    "         3 addtional run of fit_one_cycle complete\n",
    "\n",
    "    epoch \ttrain_loss \tvalid_loss \taccuracy \ttime\n",
    "        0 \t1.672191 \t1.643600 \t0.661175 \t3:02:40\n",
    "\n",
    "         4 addtional run of fit_one_cycle complete\n",
    "    completed 4 new training epochs\n",
    "    completed 7 total training epochs\n",
    "\n",
    "Output from next 4 runs:\n",
    "\n",
    "    loaded existing learner from /home/seth/mimic/mimic_lm_fine_tuned_7\n",
    "\n",
    "    epoch \ttrain_loss \tvalid_loss \taccuracy \ttime\n",
    "        0 \t1.701067 \t1.638409 \t0.661831 \t3:02:46\n",
    "\n",
    "         1 addtional run of fit_one_cycle complete\n",
    "\n",
    "    epoch \ttrain_loss \tvalid_loss \taccuracy \ttime\n",
    "        0 \t1.672565 \t1.636598 \t0.662079 \t3:02:55\n",
    "\n",
    "         2 addtional run of fit_one_cycle complete\n",
    "\n",
    "    epoch \ttrain_loss \tvalid_loss \taccuracy \ttime\n",
    "        0 \t1.715523 \t1.635751 \t0.662418 \t3:03:00\n",
    "\n",
    "         3 addtional run of fit_one_cycle complete\n",
    "\n",
    "    epoch \ttrain_loss \tvalid_loss \taccuracy \ttime\n",
    "        0 \t1.682663 \t1.632025 \t0.662714 \t3:02:57\n",
    "\n",
    "         4 addtional run of fit_one_cycle complete\n",
    "    completed 4 new training epochs\n",
    "    completed 11 total training epochs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print('now testing with differnt learning rate 5e-2')\n",
    "\n",
    "# if want to continue training existing model, set to True\n",
    "# if want to start fresh from the initialized language model, set to False\n",
    "# also, make sure to remove any previously created saved states before changing\n",
    "# flag back to continue\n",
    "continue_flag = True\n",
    "########################################################\n",
    "learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)\n",
    "\n",
    "if continue_flag:\n",
    "    # mostly a duplicate of the previous cell, but necessary to make sure if just this cell\n",
    "    # is run, everything works correctly in continue mode\n",
    "    if os.path.isfile(cycles_file):\n",
    "        with open(cycles_file, 'rb') as f:\n",
    "            prev_cycles = pickle.load(f)\n",
    "        print('This model has been trained for', prev_cycles, 'epochs already')    \n",
    "    file = lm_base_file + str(prev_cycles)\n",
    "    learner_file = base_path/file\n",
    "    if os.path.isfile(str(learner_file) + '.pth'):\n",
    "        learn.load(learner_file)\n",
    "        learn.unfreeze()\n",
    "        print('loaded existing learner from', str(learner_file))\n",
    "    else:\n",
    "        # should not continue as could not find specified file\n",
    "        print('existing learner file (', learner_file, 'not found')\n",
    "        assert(False)\n",
    "else:\n",
    "    prev_cycles = 0\n",
    "\n",
    "########################################################\n",
    "# set this to how many additional cycles you want to run\n",
    "########################################################\n",
    "num_cycles = 4\n",
    "########################################################\n",
    "\n",
    "# learn.fit_one_cycle(4, 5e-2, moms=(0.8,0.7))\n",
    "# print('    ', n + 1, 'addtional run of fit_one_cycle complete')\n",
    "# file = lm_base_file + str(prev_cycles + n + 1)\n",
    "# learner_file = base_path/file\n",
    "# learn.save(learner_file)\n",
    "# with open(cycles_file, 'wb') as f:\n",
    "#     pickle.dump(prev_cycles + n + 1, f)\n",
    "# release_mem()\n",
    "\n",
    "\n",
    "for n in range(num_cycles):\n",
    "    learn.fit_one_cycle(1, 5e-2, moms=(0.8,0.7))\n",
    "    print('    ', n + 1, 'addtional run of fit_one_cycle complete')\n",
    "    file = lm_base_file + str(prev_cycles + n + 1)\n",
    "    learner_file = base_path/file\n",
    "    learn.save(learner_file)\n",
    "    with open(cycles_file, 'wb') as f:\n",
    "        pickle.dump(prev_cycles + n + 1, f)\n",
    "    release_mem()\n",
    "    \n",
    "print('completed', num_cycles, 'new training epochs')\n",
    "print('completed', num_cycles + prev_cycles, 'total training epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print('now testing with multiple epochs and learning rate of 1e-3')\n",
    "num_cycles = 4\n",
    "prev_cycles = 4\n",
    "\n",
    "\n",
    "print('This model has been trained for', prev_cycles, 'epochs already')    \n",
    "file = lm_base_file + str(prev_cycles)\n",
    "learner_file = base_path/file\n",
    "learn.load(learner_file)\n",
    "learn.unfreeze()\n",
    "print('loaded existing learner from', str(learner_file))\n",
    "\n",
    "\n",
    "learn.fit_one_cycle(num_cycles, 1e-3, moms=(0.8,0.7))\n",
    "file = lm_base_file + str(prev_cycles + num_cycles + 1)\n",
    "learner_file = base_path/file\n",
    "learn.save(learner_file)\n",
    "release_mem()\n",
    "    \n",
    "print('completed', num_cycles, 'new training epochs')\n",
    "print('completed', num_cycles + prev_cycles, 'total training epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now testing with multiple epochs and learning rate of 0.001\n",
      "This model has been trained for 4 epochs already\n",
      "loaded existing learner from /home/seth/mimic/mimic_lm_fine_tuned_4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.719432</td>\n",
       "      <td>1.848607</td>\n",
       "      <td>0.624292</td>\n",
       "      <td>12:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.737586</td>\n",
       "      <td>1.826098</td>\n",
       "      <td>0.628307</td>\n",
       "      <td>12:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.610998</td>\n",
       "      <td>1.810599</td>\n",
       "      <td>0.632050</td>\n",
       "      <td>12:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.569480</td>\n",
       "      <td>1.811883</td>\n",
       "      <td>0.632433</td>\n",
       "      <td>12:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed 4 new training epochs\n",
      "completed 8 total training epochs\n",
      "now testing with multiple epochs and learning rate of 0.005\n",
      "This model has been trained for 4 epochs already\n",
      "loaded existing learner from /home/seth/mimic/mimic_lm_fine_tuned_4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.875421</td>\n",
       "      <td>1.969297</td>\n",
       "      <td>0.604634</td>\n",
       "      <td>12:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.944320</td>\n",
       "      <td>1.933842</td>\n",
       "      <td>0.609719</td>\n",
       "      <td>12:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.735338</td>\n",
       "      <td>1.828000</td>\n",
       "      <td>0.626465</td>\n",
       "      <td>12:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.597683</td>\n",
       "      <td>1.796420</td>\n",
       "      <td>0.633234</td>\n",
       "      <td>12:58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed 4 new training epochs\n",
      "completed 8 total training epochs\n",
      "now testing with multiple epochs and learning rate of 0.01\n",
      "This model has been trained for 4 epochs already\n",
      "loaded existing learner from /home/seth/mimic/mimic_lm_fine_tuned_4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.192642</td>\n",
       "      <td>2.164820</td>\n",
       "      <td>0.576094</td>\n",
       "      <td>12:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.147991</td>\n",
       "      <td>2.111975</td>\n",
       "      <td>0.583414</td>\n",
       "      <td>12:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.953963</td>\n",
       "      <td>1.922985</td>\n",
       "      <td>0.611166</td>\n",
       "      <td>12:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.779928</td>\n",
       "      <td>1.854817</td>\n",
       "      <td>0.623151</td>\n",
       "      <td>12:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed 4 new training epochs\n",
      "completed 8 total training epochs\n",
      "now testing with multiple epochs and learning rate of 0.05\n",
      "This model has been trained for 4 epochs already\n",
      "loaded existing learner from /home/seth/mimic/mimic_lm_fine_tuned_4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.263784</td>\n",
       "      <td>4.149484</td>\n",
       "      <td>0.380039</td>\n",
       "      <td>12:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10.339797</td>\n",
       "      <td>7.657226</td>\n",
       "      <td>0.057768</td>\n",
       "      <td>12:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.237694</td>\n",
       "      <td>4.974317</td>\n",
       "      <td>0.282988</td>\n",
       "      <td>12:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.493209</td>\n",
       "      <td>4.284114</td>\n",
       "      <td>0.388155</td>\n",
       "      <td>12:58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed 4 new training epochs\n",
      "completed 8 total training epochs\n",
      "now testing with multiple epochs and learning rate of 0.1\n",
      "This model has been trained for 4 epochs already\n",
      "loaded existing learner from /home/seth/mimic/mimic_lm_fine_tuned_4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.972384</td>\n",
       "      <td>4.839313</td>\n",
       "      <td>0.313887</td>\n",
       "      <td>12:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.960479</td>\n",
       "      <td>5.945019</td>\n",
       "      <td>0.075366</td>\n",
       "      <td>12:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.908723</td>\n",
       "      <td>5.903868</td>\n",
       "      <td>0.079820</td>\n",
       "      <td>12:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.928491</td>\n",
       "      <td>5.894521</td>\n",
       "      <td>0.079889</td>\n",
       "      <td>12:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed 4 new training epochs\n",
      "completed 8 total training epochs\n"
     ]
    }
   ],
   "source": [
    "num_cycles = 4\n",
    "prev_cycles = 4\n",
    "\n",
    "for lr in [1e-3, 5e-3, 1e-2, 5e-2, 1e-1]:\n",
    "    print('now testing with multiple epochs and learning rate of', lr)\n",
    "    print('This model has been trained for', prev_cycles, 'epochs already')    \n",
    "    file = lm_base_file + str(prev_cycles)\n",
    "    learner_file = base_path/file\n",
    "    learn.load(learner_file)\n",
    "    learn.unfreeze()\n",
    "    print('loaded existing learner from', str(learner_file))\n",
    "\n",
    "\n",
    "    learn.fit_one_cycle(num_cycles, lr, moms=(0.8,0.7))\n",
    "    file = lm_base_file + str(prev_cycles + num_cycles + 1)\n",
    "    learner_file = base_path/file\n",
    "    learn.save(learner_file)\n",
    "    release_mem()\n",
    "\n",
    "    print('completed', num_cycles, 'new training epochs')\n",
    "    print('completed', num_cycles + prev_cycles, 'total training epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 9:54:16 <p><table style='width:375px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "    <th>time</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>0</th>\n",
       "    <th>1.926960</th>\n",
       "    <th>1.832659</th>\n",
       "    <th>0.627496</th>\n",
       "    <th>1:14:14</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>1.808083</th>\n",
       "    <th>1.755725</th>\n",
       "    <th>0.637424</th>\n",
       "    <th>1:14:15</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>2</th>\n",
       "    <th>1.747903</th>\n",
       "    <th>1.697741</th>\n",
       "    <th>0.645431</th>\n",
       "    <th>1:14:15</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>3</th>\n",
       "    <th>1.714081</th>\n",
       "    <th>1.652703</th>\n",
       "    <th>0.652703</th>\n",
       "    <th>1:14:19</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>4</th>\n",
       "    <th>1.637801</th>\n",
       "    <th>1.602961</th>\n",
       "    <th>0.660170</th>\n",
       "    <th>1:14:15</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>5</th>\n",
       "    <th>1.596906</th>\n",
       "    <th>1.553225</th>\n",
       "    <th>0.668557</th>\n",
       "    <th>1:14:14</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>6</th>\n",
       "    <th>1.572020</th>\n",
       "    <th>1.519172</th>\n",
       "    <th>0.674477</th>\n",
       "    <th>1:14:26</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>7</th>\n",
       "    <th>1.517364</th>\n",
       "    <th>1.510010</th>\n",
       "    <th>0.676342</th>\n",
       "    <th>1:14:14</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jupyter/mimic/mimic_lm_fine_tuned_3.pth']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn_pattern = lm_base_file + '*'\n",
    "training_files = glob.glob(str(base_path/fn_pattern))\n",
    "training_files.sort()\n",
    "training_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to load the last file\n",
    "learn.load(training_files[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For confirmation, she underwent CTA of the lung which was negative for pulmonary embolism \n",
      "  but showed no PE , but did show some pulmonary edema . She has had \n",
      "  some mild dyspnea on exertion but has improved . She was brought to the \n",
      "  ED for further evaluation\n",
      "For confirmation, she underwent CTA of the lung which was negative for pulmonary embolism or dissection . \n",
      "  She was extubated today and given 1 unit of prbcs for Hct of 24 . She is \n",
      "  afebrile , HR in the 120s , BP stable . She is\n"
     ]
    }
   ],
   "source": [
    "# test the language generation capabilities of this model (not the point, but is interesting)\n",
    "TEXT = \"For confirmation, she underwent CTA of the lung which was negative for pulmonary embolism\"\n",
    "N_WORDS = 40\n",
    "N_SENTENCES = 2\n",
    "print(\"\\n\".join(learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder(enc_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the encoder:\n",
    "\n",
    "```python\n",
    "learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)\n",
    "learn.load_encoder(enc_file)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now based on our language model, train a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_file = 'mimic_cl.pickle'\n",
    "filename = base_path/class_file\n",
    "\n",
    "if os.path.isfile(filename):\n",
    "    data_cl = load_data(base_path, file, bs=bs)\n",
    "else:\n",
    "    data_cl = (TextList.from_df(df, cols='', vocab=data_lm.vocab)\n",
    "               #grab all the text files in path\n",
    "               .split_by_folder(valid='test')\n",
    "               #split by train and valid folder (that only keeps 'train' and 'test' so no need to filter)\n",
    "               .label_from_folder(classes=['neg', 'pos'])\n",
    "               #label them all with their folders\n",
    "               .databunch(bs=bs))\n",
    "\n",
    "data_cl.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-c42a15b2c7cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cl.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.CATEGORY.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1676"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.DESCRIPTION.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(filename):\n",
    "    data_lm = load_data(base_path, file, bs=bs)\n",
    "else:\n",
    "    data_lm = (TextList.from_df(df, 'texts.csv', cols='TEXT')\n",
    "               #df has several columns; actual text is in column TEXT\n",
    "               .split_by_rand_pct(valid_pct=valid_pct, seed=seed)\n",
    "               #We randomly split and keep 10% for validation\n",
    "               .label_from_df(cols='DESCRIPTION')\n",
    "               #We want to do a language model so we label accordingly\n",
    "               .databunch(bs=bs))\n",
    "    data_lm.save(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the version from the original example\n",
    "```python\n",
    "data_clas = (TextList.from_folder(path, vocab=data_lm.vocab)\n",
    "             #grab all the text files in path\n",
    "             .split_by_folder(valid='test')\n",
    "             #split by train and valid folder (that only keeps 'train' and 'test' so no need to filter)\n",
    "             .label_from_folder(classes=['neg', 'pos'])\n",
    "             #label them all with their folders\n",
    "             .databunch(bs=bs))\n",
    "\n",
    "data_clas.save('data_clas.pkl')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(data_cl, AWD_LSTM, drop_mult=0.5)\n",
    "learn.load_encoder(enc_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change learning rate based on results from the above plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, 2e-2, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
