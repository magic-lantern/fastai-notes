{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using FAST.AI for NLP\n",
    "\n",
    "Exploring the MIMIC III data set medical notes.\n",
    "\n",
    "Tried working with the full dataset, but almost every training step takes many hours (~13 for initial training), predicted 14+ per epoch for fine tuning.\n",
    "\n",
    "Instead will try to work with just 10% sample... Not sure that will work though\n",
    "\n",
    "A few notes:\n",
    "* See https://docs.fast.ai/text.transform.html#Tokenizer for details on what various artificial tokens (e.g xxup, xxmaj, etc.) mean\n",
    "* To view nicely formatted documentation on the fastai library, run commands like: ` doc(learn.lr_find)`\n",
    "\n",
    "### To Do:\n",
    "* need to evalate how changing the learning rate would alter training time\n",
    "* need to evalate how changing the learning rate would alter accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to verify that Torch can find and use your GPU, run the following code:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These next cells can be used to get an idea of the speed up provided by a GPU for some operations (from https://course.fast.ai/gpu_tutorial.html)\n",
    "```python\n",
    "import torch\n",
    "t_cpu = torch.rand(500,500,500)\n",
    "%timeit t_cpu @ t_cpu\n",
    "# separate cell \n",
    "t_gpu = torch.rand(500,500,500).cuda()\n",
    "%timeit t_gpu @ t_gpu\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas doesn't understand ~, so provide full path\n",
    "base_path = Path('/home/seth/mimic')\n",
    "\n",
    "# files used during processing - all aggregated here\n",
    "notes_file = base_path/'noteevents.pickle'\n",
    "lm_file = 'mimic_lm.pickle' # actual file is at base_path/lm_file but due to fastai function, have to pass file name separately\n",
    "init_model_file = base_path/'mimic_fit_head'\n",
    "cycles_file = base_path/'num_iterations.pickle'\n",
    "lm_base_file = 'mimic_lm_fine_tuned_'\n",
    "enc_file = 'mimic_fine_tuned_enc'\n",
    "class_file = 'mimic_cl.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if this doesn't free memory, can restart Python kernel.\n",
    "# if that still doesn't work, try OS items mentioned here: https://docs.fast.ai/dev/gpu.html\n",
    "def release_mem():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this to see what has already been imported\n",
    "#whos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Random Number seed for repeatability; set Batch Size to control GPU memory\n",
    "\n",
    "See **\"Performance notes\"** section below for how setting batch size impacts GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "# previously used 48; worked fine but never seemed to use even half of GPU memory; 64 still on the small side\n",
    "bs=48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While parsing a CSV and converting to a dataframe is pretty fast, loading a pickle file is much faster.\n",
    "\n",
    "For load time and size comparison:\n",
    "* `NOTEEVENTS.csv` is ~ 3.8GB in size\n",
    "  ```\n",
    "  CPU times: user 51.2 s, sys: 17.6 s, total: 1min 8s\n",
    "  Wall time: 1min 47s\n",
    "  ```\n",
    "* `noteevents.pickle` is ~ 3.7 GB in size\n",
    "  ```\n",
    "  CPU times: user 2.28 s, sys: 3.98 s, total: 6.26 s\n",
    "  Wall time: 6.26 s\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading noteevnt pickle file\n",
      "CPU times: user 1.24 s, sys: 1.27 s, total: 2.51 s\n",
      "Wall time: 3.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "orig_df = pd.DataFrame()\n",
    "if os.path.isfile(notes_file):\n",
    "    print('Loading noteevnt pickle file')\n",
    "    orig_df = pd.read_pickle(notes_file)\n",
    "else:\n",
    "    print('Could not find noteevent pickle file; creating it')\n",
    "    # run this the first time to covert CSV to Pickle file\n",
    "    orig_df = pd.read_csv(base_path/'NOTEEVENTS.csv', low_memory=False, memory_map=True)\n",
    "    orig_df.to_pickle(notes_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to data set size and performance reasons, working with a 10% sample. Use same random see to get same results from subsequent runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = orig_df.sample(frac=0.1, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to free up some memory\n",
    "# orig_df = None\n",
    "# del orig_df\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df: 982130368\n"
     ]
    }
   ],
   "source": [
    "from pympler import asizeof\n",
    "print('df:', asizeof.asizeof(df))\n",
    "#print('orig_df:', asizeof.asizeof(orig_df))\n",
    "#print('data_lm:', asizeof.asizeof(data_lm, detail=1))\n",
    "#print asizeof.asized(obj, detail=1).format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>CHARTTIME</th>\n",
       "      <th>STORETIME</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>CGID</th>\n",
       "      <th>ISERROR</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1292716</th>\n",
       "      <td>1295263</td>\n",
       "      <td>2549</td>\n",
       "      <td>159440.0</td>\n",
       "      <td>2132-04-02</td>\n",
       "      <td>2132-04-02 13:09:00</td>\n",
       "      <td>2132-04-02 13:35:00</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>18566.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CCU NSG TRANSFER SUMMARY UPDATE: RESP FAILURE\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160271</th>\n",
       "      <td>1175599</td>\n",
       "      <td>29621</td>\n",
       "      <td>190624.0</td>\n",
       "      <td>2149-02-23</td>\n",
       "      <td>2149-02-23 03:27:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Radiology</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[**2149-2-23**] 3:27 AM\\n CHEST (PORTABLE AP) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1549380</th>\n",
       "      <td>1555118</td>\n",
       "      <td>22384</td>\n",
       "      <td>142591.0</td>\n",
       "      <td>2185-03-26</td>\n",
       "      <td>2185-03-26 17:58:00</td>\n",
       "      <td>2185-03-26 18:01:00</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>16985.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Respiratory Care\\nPt remains intubated (#7.5 E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7474</th>\n",
       "      <td>5743</td>\n",
       "      <td>690</td>\n",
       "      <td>152820.0</td>\n",
       "      <td>2182-09-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2182-9-12**]       Dischar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014768</th>\n",
       "      <td>2023163</td>\n",
       "      <td>25560</td>\n",
       "      <td>156143.0</td>\n",
       "      <td>2154-11-18</td>\n",
       "      <td>2154-11-18 10:44:00</td>\n",
       "      <td>2154-11-18 17:08:00</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>16888.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Neonatology\\nOn exam pink active non-dysmorphi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ROW_ID  SUBJECT_ID   HADM_ID   CHARTDATE            CHARTTIME  \\\n",
       "1292716  1295263        2549  159440.0  2132-04-02  2132-04-02 13:09:00   \n",
       "1160271  1175599       29621  190624.0  2149-02-23  2149-02-23 03:27:00   \n",
       "1549380  1555118       22384  142591.0  2185-03-26  2185-03-26 17:58:00   \n",
       "7474        5743         690  152820.0  2182-09-14                  NaN   \n",
       "2014768  2023163       25560  156143.0  2154-11-18  2154-11-18 10:44:00   \n",
       "\n",
       "                   STORETIME           CATEGORY          DESCRIPTION     CGID  \\\n",
       "1292716  2132-04-02 13:35:00      Nursing/other               Report  18566.0   \n",
       "1160271                  NaN          Radiology  CHEST (PORTABLE AP)      NaN   \n",
       "1549380  2185-03-26 18:01:00      Nursing/other               Report  16985.0   \n",
       "7474                     NaN  Discharge summary               Report      NaN   \n",
       "2014768  2154-11-18 17:08:00      Nursing/other               Report  16888.0   \n",
       "\n",
       "         ISERROR                                               TEXT  \n",
       "1292716      NaN  CCU NSG TRANSFER SUMMARY UPDATE: RESP FAILURE\\...  \n",
       "1160271      NaN  [**2149-2-23**] 3:27 AM\\n CHEST (PORTABLE AP) ...  \n",
       "1549380      NaN  Respiratory Care\\nPt remains intubated (#7.5 E...  \n",
       "7474         NaN  Admission Date:  [**2182-9-12**]       Dischar...  \n",
       "2014768      NaN  Neonatology\\nOn exam pink active non-dysmorphi...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ROW_ID           int64\n",
       "SUBJECT_ID       int64\n",
       "HADM_ID        float64\n",
       "CHARTDATE       object\n",
       "CHARTTIME       object\n",
       "STORETIME       object\n",
       "CATEGORY        object\n",
       "DESCRIPTION     object\n",
       "CGID           float64\n",
       "ISERROR        float64\n",
       "TEXT            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208318, 11)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train and test sets; using same random seed so subsequent runs will generate same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 1./3\n",
    "train, test = train_test_split(df, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138878, 11)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69440, 11)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to build initial version of language model; If running with full dataset, requires a **LOT** of RAM; using a **LOT** of CPU helps it to happen quickly as well\n",
    "\n",
    "Questions:\n",
    "\n",
    "* why does this only seem to use CPU? (applies to both both textclasdatabunch and textlist)\n",
    "* for 100% of the mimic noteevents data:\n",
    "  * run out of memory at 32 GB, error at 52 GB, trying 72GB now... got down to only 440MB free; if crash again, increase memory\n",
    "  * now at 20vCPU and 128GB RAM; ok up to 93%; got down to 22GB available\n",
    "  * succeeded with 20CPU and 128GB RAM...\n",
    "* try smaller batch size? will that reduce memory requirements?\n",
    "* with 10% dataset sample, it seems I could get by with perhaps 32GB system RAM\n",
    "\n",
    "For comparison:\n",
    "* 10% langauge model is ~ 1.2 GB in size\n",
    "  * Time to load existing language model:\n",
    "    ```\n",
    "    CPU times: user 3.29 s, sys: 844 ms, total: 4.14 s\n",
    "    Wall time: 12.6 s\n",
    "    ```\n",
    "  * Time to build language model:\n",
    "    ```\n",
    "    CPU times: user 36.9 s, sys: 8.56 s, total: 45.4 s\n",
    "    Wall time: 3min 27s\n",
    "    ```\n",
    "* 100% language model is...\n",
    "  * Time to load existing language model:\n",
    "  * Time to build language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading existing langauge model\n",
      "CPU times: user 2.04 s, sys: 640 ms, total: 2.68 s\n",
      "Wall time: 2.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tmpfile = base_path/lm_file\n",
    "\n",
    "if os.path.isfile(tmpfile):\n",
    "    print('loading existing langauge model')\n",
    "    data_lm = load_data(base_path, lm_file, bs=bs)\n",
    "else:\n",
    "    print('creating new language model')\n",
    "    data_lm = (TextList.from_df(df, 'texts.csv', cols='TEXT')\n",
    "               #df has several columns; actual text is in column TEXT\n",
    "               .split_by_rand_pct(valid_pct=0.1, seed=seed)\n",
    "               #We randomly split and keep 10% for validation\n",
    "               .label_for_lm()\n",
    "               #We want to do a language model so we label accordingly\n",
    "               .databunch(bs=bs))\n",
    "    data_lm.save(tmpfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If need to view more data, run appropriate line to make display wider/show more columns...\n",
    "```python\n",
    "# default 20\n",
    "pd.get_option('display.max_columns')\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.max_columns', None) # show all\n",
    "# default 50\n",
    "pd.get_option('display.max_colwidth')\n",
    "pd.set_option('display.max_colwidth', -1) # show all\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>pacs . xxmaj bp went back to 150 / 80 . xxup hr varies between 80s to low 90s at rest up to 1-teens with activity . xxup bp varies more widely between 1-teens / 70s at rest up to 170 / 90s with activity . xxmaj she continues on dilt 90 mg po qid . xxmaj she was xxup k+ replaced today . \\n  xxup resp : xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>spine , xxup trauma ( xxup with xxup flex &amp; xxup ext ) ; t - l xxup spine 3 ' xxup film xxup ap &amp; xxup lat xxmaj clip # [ * * xxmaj clip xxmaj number ( xxmaj radiology ) xxunk * * ] \\n  xxmaj reason : s / p xxup mvc , s / p xxup mvc \\n  xxrep 78 _ \\n  [</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>views , xxup pa xxup and xxup lateral \\n \\n  xxmaj history of xxup cabg . \\n \\n  xxmaj status post xxup cabg . xxup picc line is in mid xxup svc . xxmaj the lungs are clear . xxmaj no \\n  pneumothorax or pleural effusion . xxmaj there is cardiomegaly but no evidence for \\n  xxup chf . \\n \\n \\n  xxbos [ *</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>) xxmaj tablet , xxmaj delayed xxmaj release ( xxup e.c. ) xxup po xxup tid ( 3 times a day ) . \\n  4 . xxmaj calcium xxmaj acetate 667 mg xxmaj capsule xxmaj sig : xxmaj three ( 3 ) xxmaj capsule xxup po xxup tid \\n  w / xxup meals ( 3 xxup times a xxup day xxup with xxup meals ) . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>has been \\n  pumping while in [ * * xxmaj hospital1 468 * * ] , and in touch with lactation \\n  consultant about low production of milk . xxmaj abdomen is soft , \\n  pink , active bowel sounds , no loops , xxup ag stable . xxmaj voiding , no \\n  stool this shift . xxmaj no spits , min residuals . tolerating \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lm.show_batch()\n",
    "# how to look at original version of text\n",
    "#df[df['TEXT'].str.contains('being paralyzed were discussed', case=False)].TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as of June 2019, this automatically loads and initializes the model based on WT103 from\n",
    "# https://s3.amazonaws.com/fast-ai-modelzoo/wt103-fwd.tgz; will auto download if not already on disk\n",
    "learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Learning rate graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot(skip_end=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial model training\n",
    "\n",
    "Time to run:\n",
    "\n",
    "* Full data set took about 13 hours using the Nvidia P1000\n",
    "* Full data set was predicted to take about 25 hours with the T4\n",
    "* 10% data took about 1 hour (1:08) using the Nvidia P1000\n",
    "* 10% data is predicted to take about 2.5 hour (actual 2:42) using the Nvidia GTX 1060\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded learner\n"
     ]
    }
   ],
   "source": [
    "# no idea how long nor how much resources this will take\n",
    "# not sure 1e-2 is the right learning rate; maybe 1e-1 or between 1e-2 and 1e-1\n",
    "# using t4\n",
    "# progress bar says this will take around 24 hours... ran for about 52 minutes\n",
    "# gpustat/nvidia-smi indicates currently only using about 5GB of GPU RAM\n",
    "# using p100\n",
    "# progress bar says this will take around 12 hours; took 13:16\n",
    "# at start GPU using about 5GB RAM\n",
    "# after about 8 hours GPU using about 7.5GB RAM.\n",
    "# looks like I could increase batch size...\n",
    "# with bs=64, still only seems to be using about 7GB GPU RAM after running for 15 minutes. \n",
    "# will check after a bit, but likely can increase batch size further\n",
    "\n",
    "if os.path.isfile(str(init_model_file) + '.pth'):\n",
    "    learn.load(init_model_file)\n",
    "    print('loaded learner')\n",
    "else:\n",
    "    learn.fit_one_cycle(1, 5e-2, moms=(0.8,0.7))\n",
    "    learn.save(init_model_file)\n",
    "    print('generated new learner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "continue from initial training - reload in case just want to continue processing from here.\n",
    "\n",
    "As an FYI pytorch automatically appends .pth to the filename, you cannot provide it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)\n",
    "#learn.load(init_model_file)\n",
    "#print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>xxbos 88 m with h / o xxup dm2 , xxup htn , hyperlipidemia , xxup chf , s /</td>\n",
       "      <td>p xxup cva , fungal bladder \\n  mass and recurrent utis presents from nursing home 3x vomiting this xxup</td>\n",
       "      <td>p xxup cabg , xxup xxup cancer xxmaj , \\n  xxup , with xxup home . / , shift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>clear to coarse throughout . xxmaj suctioned for moderate amounts thick white secretions . xxmaj remains on xxmaj pressure support</td>\n",
       "      <td>ventilation 5 / 5 / 40 % maintaining sats 100 % . xxup rr in 20 's . \\n \\n</td>\n",
       "      <td>. . - 5 . 5 % . xxup &gt; % . xxmaj rr 30 the . . xxmaj xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>/ ml \\n  [ * * 2148 - 5 - 1 * * ] 01:32 xxup am \\n</td>\n",
       "      <td>xxup wbc \\n  10.1 k / ul \\n  [ * * 2148 - 5 - 1 * *</td>\n",
       "      <td>xxup ct \\n  7.7 \\n  / ul \\n  [ * * 2115 - 12 - 25 *</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\\n  urine for a and c&amp;s sent . xxup ua is positive for 159 rbc \\n  s and</td>\n",
       "      <td>wbc \\n  and xxunk bacteria . xxmaj vanco level drawn post dialysis = 12.9 , 2 hours post \\n</td>\n",
       "      <td>\\n  xxmaj xxmaj \\n  . xxmaj urine level was at - . [ . xxmaj units .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxup md \\n \\n  xxup gu xxup lasix xxup x1 xxup eves [ * * xxmaj name8 ( xxup</td>\n",
       "      <td>md ) 103 * * ] xxup well xxup good u / o \\n \\n  xxup labs xxup bs</td>\n",
       "      <td>* * * ] xxup md xxup md xxup / o xxup xxup id : pending : wnl xxup drip</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model has been trained for 7 epochs already\n"
     ]
    }
   ],
   "source": [
    "prev_cycles = 0\n",
    "\n",
    "if os.path.isfile(cycles_file):\n",
    "    with open(cycles_file, 'rb') as f:\n",
    "        prev_cycles = pickle.load(f)\n",
    "print('This model has been trained for', prev_cycles, 'epochs already')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now fine tune language model\n",
    "\n",
    "Performance notes w/P100 GPU:\n",
    "\n",
    "* at batch size of 128 takes about 1:14:00 per epoch; GPU usage is about 14GB; RAM usage is about 10GB\n",
    "* at batch size of 96 takes about 1:17:00 per epoch; GPU usage is about  9GB; RAM usage is about 10GB\n",
    "* at batch size of 48 takes about 1:30:00 per epoch; GPU usage is about  5GB; RAM usage is about 10GB\n",
    "\n",
    "With `learn.fit_one_cycle(8, 5e-3, moms=(0.8,0.7))` (8 cycles)\n",
    "* gets from about 62.7% accuracy to 67.6% accuracy\n",
    "* Total time: 9:54:16\n",
    "\n",
    "\n",
    "Output from first 3 runs:\n",
    "\n",
    "    epoch \ttrain_loss \tvalid_loss \taccuracy \ttime\n",
    "        0 \t1.828720 \t1.741310 \t0.646276 \t3:03:05\n",
    "\n",
    "     1 addtional run of fit_one_cycle complete\n",
    "\n",
    "    epoch \ttrain_loss \tvalid_loss \taccuracy \ttime\n",
    "        0 \t1.736914 \t1.701096 \t0.652299 \t3:03:00\n",
    "\n",
    "     2 addtional run of fit_one_cycle complete\n",
    "\n",
    "    epoch \ttrain_loss \tvalid_loss \taccuracy \ttime\n",
    "        0 \t1.699437 \t1.677218 \t0.655742 \t3:03:02\n",
    "\n",
    "     3 addtional run of fit_one_cycle complete\n",
    "    completed 3 new training epochs\n",
    "    completed 3 total training epochs\n",
    "    \n",
    "Output from next 4 runs:\n",
    "\n",
    "    loaded existing learner from /home/seth/mimic/mimic_lm_fine_tuned_3\n",
    "\n",
    "    epoch \ttrain_loss \tvalid_loss \taccuracy \ttime\n",
    "        0 \t1.693833 \t1.664847 \t0.658170 \t3:03:05\n",
    "\n",
    "         1 addtional run of fit_one_cycle complete\n",
    "\n",
    "    epoch \ttrain_loss \tvalid_loss \taccuracy \ttime\n",
    "        0 \t1.745765 \t1.653829 \t0.659691 \t3:02:57\n",
    "\n",
    "         2 addtional run of fit_one_cycle complete\n",
    "\n",
    "    epoch \ttrain_loss \tvalid_loss \taccuracy \ttime\n",
    "        0 \t1.741647 \t1.648660 \t0.660596 \t3:02:53\n",
    "\n",
    "         3 addtional run of fit_one_cycle complete\n",
    "\n",
    "    epoch \ttrain_loss \tvalid_loss \taccuracy \ttime\n",
    "        0 \t1.672191 \t1.643600 \t0.661175 \t3:02:40\n",
    "\n",
    "         4 addtional run of fit_one_cycle complete\n",
    "    completed 4 new training epochs\n",
    "    completed 7 total training epochs\n",
    "\n",
    "Output from next 4 runs:\n",
    "\n",
    "    loaded existing learner from /home/seth/mimic/mimic_lm_fine_tuned_7\n",
    "\n",
    "    epoch \ttrain_loss \tvalid_loss \taccuracy \ttime\n",
    "        0 \t1.701067 \t1.638409 \t0.661831 \t3:02:46\n",
    "\n",
    "         1 addtional run of fit_one_cycle complete\n",
    "\n",
    "    epoch \ttrain_loss \tvalid_loss \taccuracy \ttime\n",
    "        0 \t1.672565 \t1.636598 \t0.662079 \t3:02:55\n",
    "\n",
    "         2 addtional run of fit_one_cycle complete\n",
    "\n",
    "    epoch \ttrain_loss \tvalid_loss \taccuracy \ttime\n",
    "        0 \t1.715523 \t1.635751 \t0.662418 \t3:03:00\n",
    "\n",
    "         3 addtional run of fit_one_cycle complete\n",
    "\n",
    "    epoch \ttrain_loss \tvalid_loss \taccuracy \ttime\n",
    "        0 \t1.682663 \t1.632025 \t0.662714 \t3:02:57\n",
    "\n",
    "         4 addtional run of fit_one_cycle complete\n",
    "    completed 4 new training epochs\n",
    "    completed 11 total training epochs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded existing learner from /home/seth/mimic/mimic_lm_fine_tuned_7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.701067</td>\n",
       "      <td>1.638409</td>\n",
       "      <td>0.661831</td>\n",
       "      <td>3:02:46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1 addtional run of fit_one_cycle complete\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.672565</td>\n",
       "      <td>1.636598</td>\n",
       "      <td>0.662079</td>\n",
       "      <td>3:02:55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     2 addtional run of fit_one_cycle complete\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.715523</td>\n",
       "      <td>1.635751</td>\n",
       "      <td>0.662418</td>\n",
       "      <td>3:03:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     3 addtional run of fit_one_cycle complete\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.682663</td>\n",
       "      <td>1.632025</td>\n",
       "      <td>0.662714</td>\n",
       "      <td>3:02:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     4 addtional run of fit_one_cycle complete\n",
      "completed 4 new training epochs\n",
      "completed 11 total training epochs\n"
     ]
    }
   ],
   "source": [
    "# if want to continue training existing model, set to True\n",
    "# if want to start fresh from the initialized language model, set to False\n",
    "# also, make sure to remove any previously created saved states before changing\n",
    "# flag back to continue\n",
    "continue_flag = True\n",
    "########################################################\n",
    "learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)\n",
    "\n",
    "if continue_flag:\n",
    "    file = lm_base_file + str(prev_cycles)\n",
    "    learner_file = base_path/file\n",
    "    if os.path.isfile(str(learner_file) + '.pth'):\n",
    "        learn.load(learner_file)\n",
    "        print('loaded existing learner from', str(learner_file))\n",
    "    else:\n",
    "        # should not continue as could not find specified file\n",
    "        print('existing learner file not found')\n",
    "        assert(False)\n",
    "else:\n",
    "    prev_cycles = 0\n",
    "\n",
    "learn.unfreeze()\n",
    "\n",
    "########################################################\n",
    "# set this to how many additional cycles you want to run\n",
    "########################################################\n",
    "num_cycles = 4\n",
    "########################################################\n",
    "\n",
    "for n in range(num_cycles):\n",
    "    learn.fit_one_cycle(1, 5e-3, moms=(0.8,0.7))\n",
    "    print('    ', n + 1, 'addtional run of fit_one_cycle complete')\n",
    "    file = lm_base_file + str(prev_cycles + n + 1)\n",
    "    learner_file = base_path/file\n",
    "    learn.save(learner_file)\n",
    "    with open(cycles_file, 'wb') as f:\n",
    "        pickle.dump(prev_cycles + n + 1, f)\n",
    "    release_mem()\n",
    "    \n",
    "print('completed', num_cycles, 'new training epochs')\n",
    "print('completed', num_cycles + prev_cycles, 'total training epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4 id=\"Learner.save\" class=\"doc_header\"><code>save</code><a href=\"https://github.com/fastai/fastai/blob/master/fastai/basic_train.py#L245\" class=\"source_link\" style=\"float:right\">[source]</a><a class=\"source_link\" data-toggle=\"collapse\" data-target=\"#Learner-save-pytest\" style=\"float:right; padding-right:10px\">[test]</a></h4><blockquote><p><code>save</code>(<strong><code>file</code></strong>:<code>PathLikeOrBinaryStream</code>=<strong><em><code>None</code></em></strong>, <strong><code>return_path</code></strong>:<code>bool</code>=<strong><em><code>False</code></em></strong>, <strong><code>with_opt</code></strong>:<code>bool</code>=<strong><em><code>True</code></em></strong>)</p>\n",
       "</blockquote>\n",
       "<div class=\"collapse\" id=\"Learner-save-pytest\"><div class=\"card card-body pytest_card\"><a type=\"button\" data-toggle=\"collapse\" data-target=\"#Learner-save-pytest\" class=\"close\" aria-label=\"Close\"><span aria-hidden=\"true\">&times;</span></a><p>Tests found for <code>save</code>:</p><ul><li><code>pytest -sv tests/test_basic_train.py::test_memory</code> <a href=\"https://github.com/fastai/fastai/blob/master/tests/test_basic_train.py#L213\" class=\"source_link\" style=\"float:right\">[source]</a></li><li><code>pytest -sv tests/test_basic_train.py::test_save_load</code> <a href=\"https://github.com/fastai/fastai/blob/master/tests/test_basic_train.py#L104\" class=\"source_link\" style=\"float:right\">[source]</a></li></ul><p>To run tests please refer to this <a href=\"/dev/test.html#quick-guide\">guide</a>.</p></div></div><p>Save model and optimizer state (if <code>with_opt</code>) with <code>file</code> to <code>self.model_dir</code>. <code>file</code> can be file-like (file or buffer)</p>\n",
       "<p><a href=\"https://docs.fast.ai/basic_train.html#Learner.save\" target=\"_blank\" rel=\"noreferrer noopener\">Show in docs</a></p>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc(learn.save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 9:54:16 <p><table style='width:375px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "    <th>time</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>0</th>\n",
       "    <th>1.926960</th>\n",
       "    <th>1.832659</th>\n",
       "    <th>0.627496</th>\n",
       "    <th>1:14:14</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>1.808083</th>\n",
       "    <th>1.755725</th>\n",
       "    <th>0.637424</th>\n",
       "    <th>1:14:15</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>2</th>\n",
       "    <th>1.747903</th>\n",
       "    <th>1.697741</th>\n",
       "    <th>0.645431</th>\n",
       "    <th>1:14:15</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>3</th>\n",
       "    <th>1.714081</th>\n",
       "    <th>1.652703</th>\n",
       "    <th>0.652703</th>\n",
       "    <th>1:14:19</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>4</th>\n",
       "    <th>1.637801</th>\n",
       "    <th>1.602961</th>\n",
       "    <th>0.660170</th>\n",
       "    <th>1:14:15</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>5</th>\n",
       "    <th>1.596906</th>\n",
       "    <th>1.553225</th>\n",
       "    <th>0.668557</th>\n",
       "    <th>1:14:14</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>6</th>\n",
       "    <th>1.572020</th>\n",
       "    <th>1.519172</th>\n",
       "    <th>0.674477</th>\n",
       "    <th>1:14:26</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>7</th>\n",
       "    <th>1.517364</th>\n",
       "    <th>1.510010</th>\n",
       "    <th>0.676342</th>\n",
       "    <th>1:14:14</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jupyter/mimic/mimic_lm_fine_tuned_3.pth']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn_pattern = lm_base_file + '*'\n",
    "training_files = glob.glob(str(base_path/fn_pattern))\n",
    "training_files.sort()\n",
    "training_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to load the last file\n",
    "learn.load(training_files[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For confirmation, she underwent CTA of the lung which was negative for pulmonary embolism \n",
      "  but showed no PE , but did show some pulmonary edema . She has had \n",
      "  some mild dyspnea on exertion but has improved . She was brought to the \n",
      "  ED for further evaluation\n",
      "For confirmation, she underwent CTA of the lung which was negative for pulmonary embolism or dissection . \n",
      "  She was extubated today and given 1 unit of prbcs for Hct of 24 . She is \n",
      "  afebrile , HR in the 120s , BP stable . She is\n"
     ]
    }
   ],
   "source": [
    "# test the language generation capabilities of this model (not the point, but is interesting)\n",
    "TEXT = \"For confirmation, she underwent CTA of the lung which was negative for pulmonary embolism\"\n",
    "N_WORDS = 40\n",
    "N_SENTENCES = 2\n",
    "print(\"\\n\".join(learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder(enc_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the encoder:\n",
    "\n",
    "```python\n",
    "learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)\n",
    "learn.load_encoder(enc_file)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now based on our language model, train a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_file = 'mimic_cl.pickle'\n",
    "filename = base_path/class_file\n",
    "\n",
    "if os.path.isfile(filename):\n",
    "    data_cl = load_data(base_path, file, bs=bs)\n",
    "else:\n",
    "    data_cl = (TextList.from_df(df, cols='', vocab=data_lm.vocab)\n",
    "               #grab all the text files in path\n",
    "               .split_by_folder(valid='test')\n",
    "               #split by train and valid folder (that only keeps 'train' and 'test' so no need to filter)\n",
    "               .label_from_folder(classes=['neg', 'pos'])\n",
    "               #label them all with their folders\n",
    "               .databunch(bs=bs))\n",
    "\n",
    "data_cl.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-c42a15b2c7cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cl.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.CATEGORY.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1676"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.DESCRIPTION.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(filename):\n",
    "    data_lm = load_data(base_path, file, bs=bs)\n",
    "else:\n",
    "    data_lm = (TextList.from_df(df, 'texts.csv', cols='TEXT')\n",
    "               #df has several columns; actual text is in column TEXT\n",
    "               .split_by_rand_pct(valid_pct=0.1, seed=seed)\n",
    "               #We randomly split and keep 10% for validation\n",
    "               .label_from_df(cols='DESCRIPTION')\n",
    "               #We want to do a language model so we label accordingly\n",
    "               .databunch(bs=bs))\n",
    "    data_lm.save(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the version from the original example\n",
    "```python\n",
    "data_clas = (TextList.from_folder(path, vocab=data_lm.vocab)\n",
    "             #grab all the text files in path\n",
    "             .split_by_folder(valid='test')\n",
    "             #split by train and valid folder (that only keeps 'train' and 'test' so no need to filter)\n",
    "             .label_from_folder(classes=['neg', 'pos'])\n",
    "             #label them all with their folders\n",
    "             .databunch(bs=bs))\n",
    "\n",
    "data_clas.save('data_clas.pkl')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(data_cl, AWD_LSTM, drop_mult=0.5)\n",
    "learn.load_encoder(enc_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change learning rate based on results from the above plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, 2e-2, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
