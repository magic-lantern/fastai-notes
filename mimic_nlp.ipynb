{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using FAST.AI for NLP\n",
    "\n",
    "Exploring the MIMIC III data set medical notes.\n",
    "\n",
    "Tried working with the full dataset, but almost every training step takes many hours (~13 for initial training), predicted 14+ per epoch for fine tuning.\n",
    "\n",
    "Instead will try to work with just 10% sample... Not sure that will work though\n",
    "\n",
    "A few notes:\n",
    "* See https://docs.fast.ai/text.transform.html#Tokenizer for details on what various artificial tokens (e.g xxup, xxmaj, etc.) mean\n",
    "* To view nicely formatted documentation on the fastai library, run commands like: ` doc(learn.lr_find)`\n",
    "\n",
    "### To Do:\n",
    "* need to evalate how changing the learning rate would alter training time\n",
    "* need to evalate how changing the learning rate would alter accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "import gc\n",
    "from pympler import asizeof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to verify that Torch can find and use your GPU, run the following code:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These next cells can be used to get an idea of the speed up provided by a GPU for some operations (from https://course.fast.ai/gpu_tutorial.html)\n",
    "```python\n",
    "import torch\n",
    "t_cpu = torch.rand(500,500,500)\n",
    "%timeit t_cpu @ t_cpu\n",
    "# separate cell \n",
    "t_gpu = torch.rand(500,500,500).cuda()\n",
    "%timeit t_gpu @ t_gpu\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original data set too large to work with in reasonable time due to limted GPU resources\n",
    "pct_data_sample = 0.01\n",
    "# how much to hold out for validation\n",
    "valid_pct = 0.1\n",
    "\n",
    "# pandas doesn't understand ~, so provide full path\n",
    "base_path = Path.home() / 'mimic'\n",
    "\n",
    "# files used during processing - all aggregated here\n",
    "notes_file = base_path/'noteevents.pickle'\n",
    "lm_file = 'mimic_lm.pickle' # actual file is at base_path/lm_file but due to fastai function, have to pass file name separately\n",
    "init_model_file = base_path/'mimic_fit_head'\n",
    "cycles_file = base_path/'num_iterations.pickle'\n",
    "lm_base_file = 'mimic_lm_fine_tuned_'\n",
    "enc_file = 'mimic_fine_tuned_enc'\n",
    "class_file = 'mimic_cl.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if this doesn't free memory, can restart Python kernel.\n",
    "# if that still doesn't work, try OS items mentioned here: https://docs.fast.ai/dev/gpu.html\n",
    "def release_mem():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this to see what has already been imported\n",
    "#whos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Random Number seed for repeatability; set Batch Size to control GPU memory\n",
    "\n",
    "See **\"Performance notes\"** section below for how setting batch size impacts GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "# previously used 48; worked fine but never seemed to use even half of GPU memory; 64 still on the small side\n",
    "bs=96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While parsing a CSV and converting to a dataframe is pretty fast, loading a pickle file is much faster.\n",
    "\n",
    "For load time and size comparison:\n",
    "* `NOTEEVENTS.csv` is ~ 3.8GB in size\n",
    "  ```\n",
    "  CPU times: user 51.2 s, sys: 17.6 s, total: 1min 8s\n",
    "  Wall time: 1min 47s\n",
    "  ```\n",
    "* `noteevents.pickle` is ~ 3.7 GB in size\n",
    "  ```\n",
    "  CPU times: user 2.28 s, sys: 3.98 s, total: 6.26 s\n",
    "  Wall time: 6.26 s\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find noteevent pickle file; creating it\n",
      "CPU times: user 45.8 s, sys: 7.13 s, total: 53 s\n",
      "Wall time: 53.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "orig_df = pd.DataFrame()\n",
    "if os.path.isfile(notes_file):\n",
    "    print('Loading noteevent pickle file')\n",
    "    orig_df = pd.read_pickle(notes_file)\n",
    "else:\n",
    "    print('Could not find noteevent pickle file; creating it')\n",
    "    # run this the first time to covert CSV to Pickle file\n",
    "    orig_df = pd.read_csv(base_path/'NOTEEVENTS.csv', low_memory=False, memory_map=True)\n",
    "    orig_df.to_pickle(notes_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to data set size and performance reasons, working with a 10% sample. Use same random see to get same results from subsequent runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = orig_df.sample(frac=pct_data_sample, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to free up some memory\n",
    "# orig_df = None\n",
    "# del orig_df\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df: 93 MB\n"
     ]
    }
   ],
   "source": [
    "print('df:', int(asizeof.asizeof(df) / 1024 / 1024), 'MB')\n",
    "#print('orig_df:', asizeof.asizeof(orig_df))\n",
    "#print('data_lm:', asizeof.asizeof(data_lm, detail=1))\n",
    "#print asizeof.asized(obj, detail=1).format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>CHARTDATE</th>\n",
       "      <th>CHARTTIME</th>\n",
       "      <th>STORETIME</th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>CGID</th>\n",
       "      <th>ISERROR</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1292716</th>\n",
       "      <td>1295263</td>\n",
       "      <td>2549</td>\n",
       "      <td>159440.0</td>\n",
       "      <td>2132-04-02</td>\n",
       "      <td>2132-04-02 13:09:00</td>\n",
       "      <td>2132-04-02 13:35:00</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>18566.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CCU NSG TRANSFER SUMMARY UPDATE: RESP FAILURE\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160271</th>\n",
       "      <td>1175599</td>\n",
       "      <td>29621</td>\n",
       "      <td>190624.0</td>\n",
       "      <td>2149-02-23</td>\n",
       "      <td>2149-02-23 03:27:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Radiology</td>\n",
       "      <td>CHEST (PORTABLE AP)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[**2149-2-23**] 3:27 AM\\n CHEST (PORTABLE AP) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1549380</th>\n",
       "      <td>1555118</td>\n",
       "      <td>22384</td>\n",
       "      <td>142591.0</td>\n",
       "      <td>2185-03-26</td>\n",
       "      <td>2185-03-26 17:58:00</td>\n",
       "      <td>2185-03-26 18:01:00</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>16985.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Respiratory Care\\nPt remains intubated (#7.5 E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7474</th>\n",
       "      <td>5743</td>\n",
       "      <td>690</td>\n",
       "      <td>152820.0</td>\n",
       "      <td>2182-09-14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Discharge summary</td>\n",
       "      <td>Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Date:  [**2182-9-12**]       Dischar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014768</th>\n",
       "      <td>2023163</td>\n",
       "      <td>25560</td>\n",
       "      <td>156143.0</td>\n",
       "      <td>2154-11-18</td>\n",
       "      <td>2154-11-18 10:44:00</td>\n",
       "      <td>2154-11-18 17:08:00</td>\n",
       "      <td>Nursing/other</td>\n",
       "      <td>Report</td>\n",
       "      <td>16888.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Neonatology\\nOn exam pink active non-dysmorphi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ROW_ID  SUBJECT_ID   HADM_ID   CHARTDATE            CHARTTIME  \\\n",
       "1292716  1295263        2549  159440.0  2132-04-02  2132-04-02 13:09:00   \n",
       "1160271  1175599       29621  190624.0  2149-02-23  2149-02-23 03:27:00   \n",
       "1549380  1555118       22384  142591.0  2185-03-26  2185-03-26 17:58:00   \n",
       "7474        5743         690  152820.0  2182-09-14                  NaN   \n",
       "2014768  2023163       25560  156143.0  2154-11-18  2154-11-18 10:44:00   \n",
       "\n",
       "                   STORETIME           CATEGORY          DESCRIPTION     CGID  \\\n",
       "1292716  2132-04-02 13:35:00      Nursing/other               Report  18566.0   \n",
       "1160271                  NaN          Radiology  CHEST (PORTABLE AP)      NaN   \n",
       "1549380  2185-03-26 18:01:00      Nursing/other               Report  16985.0   \n",
       "7474                     NaN  Discharge summary               Report      NaN   \n",
       "2014768  2154-11-18 17:08:00      Nursing/other               Report  16888.0   \n",
       "\n",
       "         ISERROR                                               TEXT  \n",
       "1292716      NaN  CCU NSG TRANSFER SUMMARY UPDATE: RESP FAILURE\\...  \n",
       "1160271      NaN  [**2149-2-23**] 3:27 AM\\n CHEST (PORTABLE AP) ...  \n",
       "1549380      NaN  Respiratory Care\\nPt remains intubated (#7.5 E...  \n",
       "7474         NaN  Admission Date:  [**2182-9-12**]       Dischar...  \n",
       "2014768      NaN  Neonatology\\nOn exam pink active non-dysmorphi...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ROW_ID           int64\n",
       "SUBJECT_ID       int64\n",
       "HADM_ID        float64\n",
       "CHARTDATE       object\n",
       "CHARTTIME       object\n",
       "STORETIME       object\n",
       "CATEGORY        object\n",
       "DESCRIPTION     object\n",
       "CGID           float64\n",
       "ISERROR        float64\n",
       "TEXT            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20832, 11)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train and test sets; using same random seed so subsequent runs will generate same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 1./3\n",
    "train, test = train_test_split(df, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13888, 11)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6944, 11)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to build initial version of language model; If running with full dataset, requires a **LOT** of RAM; using a **LOT** of CPU helps it to happen quickly as well\n",
    "\n",
    "Questions:\n",
    "\n",
    "* why does this only seem to use CPU? (applies to both both textclasdatabunch and textlist)\n",
    "* for 100% of the mimic noteevents data:\n",
    "  * run out of memory at 32 GB, error at 52 GB, trying 72GB now... got down to only 440MB free; if crash again, increase memory\n",
    "  * now at 20vCPU and 128GB RAM; ok up to 93%; got down to 22GB available\n",
    "  * succeeded with 20CPU and 128GB RAM...\n",
    "* try smaller batch size? will that reduce memory requirements?\n",
    "* with 10% dataset sample, it seems I could get by with perhaps 32GB system RAM\n",
    "\n",
    "For comparison:\n",
    "* 10% langauge model is ~ 1.2 GB in size\n",
    "  * Time to load existing language model:\n",
    "    ```\n",
    "    CPU times: user 3.29 s, sys: 844 ms, total: 4.14 s\n",
    "    Wall time: 12.6 s\n",
    "    ```\n",
    "  * Time to build language model:\n",
    "    ```\n",
    "    CPU times: user 36.9 s, sys: 8.56 s, total: 45.4 s\n",
    "    Wall time: 3min 27s\n",
    "    ```\n",
    "* 100% language model is...\n",
    "  * Time to load existing language model:\n",
    "  * Time to build language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.69 s, sys: 2.06 s, total: 7.74 s\n",
      "Wall time: 28.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tmpfile = base_path/lm_file\n",
    "\n",
    "if os.path.isfile(tmpfile):\n",
    "    print('loading existing langauge model')\n",
    "    data_lm = load_data(base_path, lm_file, bs=bs)\n",
    "else:\n",
    "    print('creating new language model')\n",
    "    data_lm = (TextList.from_df(df, 'texts.csv', cols='TEXT')\n",
    "               #df has several columns; actual text is in column TEXT\n",
    "               .split_by_rand_pct(valid_pct=valid_pct, seed=seed)\n",
    "               #We randomly split and keep 10% for validation\n",
    "               .label_for_lm()\n",
    "               #We want to do a language model so we label accordingly\n",
    "               .databunch(bs=bs))\n",
    "    data_lm.save(tmpfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If need to view more data, run appropriate line to make display wider/show more columns...\n",
    "```python\n",
    "# default 20\n",
    "pd.get_option('display.max_columns')\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.max_columns', None) # show all\n",
    "# default 50\n",
    "pd.get_option('display.max_colwidth')\n",
    "pd.set_option('display.max_colwidth', -1) # show all\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>pacs . xxmaj bp went back to 150 / 80 . xxup hr varies between 80s to low 90s at rest up to 1-teens with activity . xxup bp varies more widely between 1-teens / 70s at rest up to 170 / 90s with activity . xxmaj she continues on dilt 90 mg po qid . xxmaj she was xxup k+ replaced today . \\n  xxup resp : xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>inr:17.6 / 35.9 / 1.6 , xxup ck / xxup ckmb / \\n  xxmaj troponin - xxup xxunk / 6 / 0.03 , xxmaj differential - xxmaj neuts:79.9 % , xxmaj lymph:10.8 % , \\n  xxmaj mono:5.8 % , xxmaj eos:3.3 % , xxmaj ca++:8.0 mg / dl , xxmaj mg++:2.0 mg / dl , xxup po4:1.8 mg / dl \\n  xxmaj assessment and xxmaj plan \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[ * * xxmaj last xxmaj name ( namepattern1 ) 1782 * * ] at 11:15 a.m. on [ * * 2181 - 2 - 20 * * ] . \\n \\n  xxbos xxmaj renal failure , acute ( xxmaj acute renal failure , xxup arf ) \\n  xxmaj assessment : \\n  xxmaj pt . anuric s / p nephrectomy . \\n  xxmaj action : \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>xxmaj name ( xxup ni ) 87 * * ] . \\n \\n  p : xxmaj fully awaken and extubate . xxmaj restart cardiac meds as soon as pt able to take po . xxmaj restart oral diabetic meds when pt taking food . xxmaj maintain c - collar at all times , under head pillow when supine . ? transfuse xxup rbc in setting of volume dependent hypotension</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>and suppport as needed . \\n \\n  xxup fen : xxmaj weight 848 g , xxunk . tf=130cc / kg / day . xxmaj enteral feeds \\n  presently at 120cc / kg / day of xxup xxunk 20cc xxmaj q4hrs , xxup pg , gavaged \\n  over 40 minutes and tol well . xxmaj no spits . xxmaj dstick 61 . xxup ivf off \\n  at</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lm.show_batch()\n",
    "# how to look at original version of text\n",
    "#df[df['TEXT'].str.contains('being paralyzed were discussed', case=False)].TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as of June 2019, this automatically loads and initializes the model based on WT103 from\n",
    "# https://s3.amazonaws.com/fast-ai-modelzoo/wt103-fwd.tgz; will auto download if not already on disk\n",
    "learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Learning rate graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot(skip_end=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial model training\n",
    "\n",
    "Time to run:\n",
    "\n",
    "* Full data set took about 13 hours using the Nvidia P1000\n",
    "* Full data set was predicted to take about 25 hours with the T4\n",
    "* 10% data took about 1 hour (1:08) using the Nvidia P1000\n",
    "* 10% data is predicted to take about 2.5 hour (actual 2:42) using the Nvidia GTX 1060\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 10:22 <p><table style='width:375px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "    <th>time</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>0</th>\n",
       "    <th>2.950593</th>\n",
       "    <th>2.859097</th>\n",
       "    <th>0.494172</th>\n",
       "    <th>05:11</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>2.547709</th>\n",
       "    <th>2.413949</th>\n",
       "    <th>0.539163</th>\n",
       "    <th>05:11</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated new learner\n"
     ]
    }
   ],
   "source": [
    "# no idea how long nor how much resources this will take\n",
    "# not sure 1e-2 is the right learning rate; maybe 1e-1 or between 1e-2 and 1e-1\n",
    "# using t4\n",
    "# progress bar says this will take around 24 hours... ran for about 52 minutes\n",
    "# gpustat/nvidia-smi indicates currently only using about 5GB of GPU RAM\n",
    "# using p100\n",
    "# progress bar says this will take around 12 hours; took 13:16\n",
    "# at start GPU using about 5GB RAM\n",
    "# after about 8 hours GPU using about 7.5GB RAM.\n",
    "# looks like I could increase batch size...\n",
    "# with bs=64, still only seems to be using about 7GB GPU RAM after running for 15 minutes. \n",
    "# will check after a bit, but likely can increase batch size further\n",
    "#\n",
    "# note about number of epochs/cycle length: Using a value of 1 does a rapid increase and\n",
    "# decrease of learning rate and end result gets almost the save result as 2 but in half\n",
    "# the time\n",
    "if os.path.isfile(str(init_model_file) + '.pth'):\n",
    "    learn.load(init_model_file)\n",
    "    print('loaded learner')\n",
    "else:\n",
    "    learn.fit_one_cycle(1, 5e-2, moms=(0.8,0.7))\n",
    "    learn.save(init_model_file)\n",
    "    print('generated new learner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "continue from initial training - reload in case just want to continue processing from here.\n",
    "\n",
    "As an FYI pytorch automatically appends .pth to the filename, you cannot provide it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)\n",
    "#learn.load(init_model_file)\n",
    "#print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj resp xxmaj care xxmaj note : \\n \\n  xxmaj pt cont trached on cool mist aerosol as</td>\n",
       "      <td>per xxmaj carevue . xxmaj lung sounds coarse dim @ bases suct xxunk th pale yellow sput . xxmaj pt</td>\n",
       "      <td>tent tolerated pt carevue . xxmaj pt sounds are , at bases . sm thick thick yellow sput . xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>: \\n  xxmaj swan - xxmaj ganz catheter with tip at the periphery of the central pulmonary artery .</td>\n",
       "      <td>\\n  xxmaj new right internal jugular venous catheter with the tip in the lower superior \\n  vena cava</td>\n",
       "      <td>. xxmaj xxmaj assessment right pleural jugular line catheter . tip tip in the right \\n  vena vena cava</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>. \\n  xxup issues : \\n  - xxup tbm , s / p xxunk and posterior tracheal splinting</td>\n",
       "      <td>via r \\n  thoracotomy [ * * 1 - 23 * * ] \\n  - subcutaneous emphysema \\n</td>\n",
       "      <td>, xxup / subclavian , * * xxmaj - 28 * * ] . xxmaj xxmaj \\n  \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\\n  30 \\n  31 \\n  xxmaj glucose \\n  156 \\n  94 \\n  75 \\n</td>\n",
       "      <td>110 \\n  85 \\n  xxmaj other labs : xxup pt / xxup ptt / xxup inr:13.6 / 32.1</td>\n",
       "      <td>medications : xxup pt / xxup ptt / xxup xxunk / \\n  / 1.2 , xxup ck / xxup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxmaj midazolam ( xxmaj versed ) - [ * * 2111 - 12 - 7 * * ] 05:30 xxup</td>\n",
       "      <td>am \\n  xxmaj fentanyl - [ * * 2111 - 12 - 7 * * ] 05:30 xxup am</td>\n",
       "      <td>xxup am \\n  xxmaj respiratory : [ * * 2139 - 4 - 26 * * ] 08:00 xxup</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model has been trained for 8 epochs already\n"
     ]
    }
   ],
   "source": [
    "prev_cycles = 0\n",
    "\n",
    "if os.path.isfile(cycles_file):\n",
    "    with open(cycles_file, 'rb') as f:\n",
    "        prev_cycles = pickle.load(f)\n",
    "print('This model has been trained for', prev_cycles, 'epochs already')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Now fine tune language model\n",
    "\n",
    "Performance notes w/P100 GPU:\n",
    "\n",
    "* at batch size of 128 takes about 1:14:00 per epoch; GPU usage is about 14GB; RAM usage is about 10GB\n",
    "* at batch size of 96 takes about 1:17:00 per epoch; GPU usage is about  9GB; RAM usage is about 10GB\n",
    "* at batch size of 48 takes about 1:30:00 per epoch; GPU usage is about  5GB; RAM usage is about 10GB\n",
    "\n",
    "With `learn.fit_one_cycle(8, 5e-3, moms=(0.8,0.7))` (8 cycles)\n",
    "* gets from about 62.7% accuracy to 67.6% accuracy\n",
    "* Total time: 9:54:16\n",
    "\n",
    "\n",
    "    epoch \ttrain_loss \tvalid_loss \taccuracy \ttime\n",
    "    0 \t1.926960 \t1.832659 \t0.627496 \t1:14:14\n",
    "    1 \t1.808083 \t1.755725 \t0.637424 \t1:14:15\n",
    "    2 \t1.747903 \t1.697741 \t0.645431 \t1:14:15\n",
    "    3 \t1.714081 \t1.652703 \t0.652703 \t1:14:19\n",
    "    4 \t1.637801 \t1.602961 \t0.660170 \t1:14:15\n",
    "    5 \t1.596906 \t1.553225 \t0.668557 \t1:14:14\n",
    "    6 \t1.572020 \t1.519172 \t0.674477 \t1:14:26\n",
    "    7 \t1.517364 \t1.510010 \t0.676342 \t1:14:14\n",
    "\n",
    "\n",
    "Output from first 3 runs:\n",
    "\n",
    "    epoch \ttrain_loss \tvalid_loss \taccuracy \ttime\n",
    "        0 \t1.828720 \t1.741310 \t0.646276 \t3:03:05\n",
    "\n",
    "     1 addtional run of fit_one_cycle complete\n",
    "\n",
    "    epoch \ttrain_loss \tvalid_loss \taccuracy \ttime\n",
    "        0 \t1.736914 \t1.701096 \t0.652299 \t3:03:00\n",
    "\n",
    "     2 addtional run of fit_one_cycle complete\n",
    "\n",
    "    epoch \ttrain_loss \tvalid_loss \taccuracy \ttime\n",
    "        0 \t1.699437 \t1.677218 \t0.655742 \t3:03:02\n",
    "\n",
    "     3 addtional run of fit_one_cycle complete\n",
    "    completed 3 new training epochs\n",
    "    completed 3 total training epochs\n",
    "    \n",
    "Output from next 4 runs:\n",
    "\n",
    "    loaded existing learner from /home/seth/mimic/mimic_lm_fine_tuned_3\n",
    "\n",
    "    epoch \ttrain_loss \tvalid_loss \taccuracy \ttime\n",
    "        0 \t1.693833 \t1.664847 \t0.658170 \t3:03:05\n",
    "\n",
    "         1 addtional run of fit_one_cycle complete\n",
    "\n",
    "    epoch \ttrain_loss \tvalid_loss \taccuracy \ttime\n",
    "        0 \t1.745765 \t1.653829 \t0.659691 \t3:02:57\n",
    "\n",
    "         2 addtional run of fit_one_cycle complete\n",
    "\n",
    "    epoch \ttrain_loss \tvalid_loss \taccuracy \ttime\n",
    "        0 \t1.741647 \t1.648660 \t0.660596 \t3:02:53\n",
    "\n",
    "         3 addtional run of fit_one_cycle complete\n",
    "\n",
    "    epoch \ttrain_loss \tvalid_loss \taccuracy \ttime\n",
    "        0 \t1.672191 \t1.643600 \t0.661175 \t3:02:40\n",
    "\n",
    "         4 addtional run of fit_one_cycle complete\n",
    "    completed 4 new training epochs\n",
    "    completed 7 total training epochs\n",
    "\n",
    "Output from next 4 runs:\n",
    "\n",
    "    loaded existing learner from /home/seth/mimic/mimic_lm_fine_tuned_7\n",
    "\n",
    "    epoch \ttrain_loss \tvalid_loss \taccuracy \ttime\n",
    "        0 \t1.701067 \t1.638409 \t0.661831 \t3:02:46\n",
    "\n",
    "         1 addtional run of fit_one_cycle complete\n",
    "\n",
    "    epoch \ttrain_loss \tvalid_loss \taccuracy \ttime\n",
    "        0 \t1.672565 \t1.636598 \t0.662079 \t3:02:55\n",
    "\n",
    "         2 addtional run of fit_one_cycle complete\n",
    "\n",
    "    epoch \ttrain_loss \tvalid_loss \taccuracy \ttime\n",
    "        0 \t1.715523 \t1.635751 \t0.662418 \t3:03:00\n",
    "\n",
    "         3 addtional run of fit_one_cycle complete\n",
    "\n",
    "    epoch \ttrain_loss \tvalid_loss \taccuracy \ttime\n",
    "        0 \t1.682663 \t1.632025 \t0.662714 \t3:02:57\n",
    "\n",
    "         4 addtional run of fit_one_cycle complete\n",
    "    completed 4 new training epochs\n",
    "    completed 11 total training epochs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print('now testing with differnt learning rate 5e-2')\n",
    "\n",
    "# if want to continue training existing model, set to True\n",
    "# if want to start fresh from the initialized language model, set to False\n",
    "# also, make sure to remove any previously created saved states before changing\n",
    "# flag back to continue\n",
    "continue_flag = True\n",
    "########################################################\n",
    "learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)\n",
    "\n",
    "if continue_flag:\n",
    "    # mostly a duplicate of the previous cell, but necessary to make sure if just this cell\n",
    "    # is run, everything works correctly in continue mode\n",
    "    if os.path.isfile(cycles_file):\n",
    "        with open(cycles_file, 'rb') as f:\n",
    "            prev_cycles = pickle.load(f)\n",
    "        print('This model has been trained for', prev_cycles, 'epochs already')    \n",
    "    file = lm_base_file + str(prev_cycles)\n",
    "    learner_file = base_path/file\n",
    "    if os.path.isfile(str(learner_file) + '.pth'):\n",
    "        learn.load(learner_file)\n",
    "        learn.unfreeze()\n",
    "        print('loaded existing learner from', str(learner_file))\n",
    "    else:\n",
    "        # should not continue as could not find specified file\n",
    "        print('existing learner file (', learner_file, 'not found')\n",
    "        assert(False)\n",
    "else:\n",
    "    prev_cycles = 0\n",
    "\n",
    "########################################################\n",
    "# set this to how many additional cycles you want to run\n",
    "########################################################\n",
    "num_cycles = 4\n",
    "########################################################\n",
    "\n",
    "# learn.fit_one_cycle(4, 5e-2, moms=(0.8,0.7))\n",
    "# print('    ', n + 1, 'addtional run of fit_one_cycle complete')\n",
    "# file = lm_base_file + str(prev_cycles + n + 1)\n",
    "# learner_file = base_path/file\n",
    "# learn.save(learner_file)\n",
    "# with open(cycles_file, 'wb') as f:\n",
    "#     pickle.dump(prev_cycles + n + 1, f)\n",
    "# release_mem()\n",
    "\n",
    "\n",
    "for n in range(num_cycles):\n",
    "    learn.fit_one_cycle(1, 5e-2, moms=(0.8,0.7))\n",
    "    print('    ', n + 1, 'addtional run of fit_one_cycle complete')\n",
    "    file = lm_base_file + str(prev_cycles + n + 1)\n",
    "    learner_file = base_path/file\n",
    "    learn.save(learner_file)\n",
    "    with open(cycles_file, 'wb') as f:\n",
    "        pickle.dump(prev_cycles + n + 1, f)\n",
    "    release_mem()\n",
    "    \n",
    "print('completed', num_cycles, 'new training epochs')\n",
    "print('completed', num_cycles + prev_cycles, 'total training epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print('now testing with multiple epochs and learning rate of 1e-3')\n",
    "num_cycles = 4\n",
    "prev_cycles = 4\n",
    "\n",
    "\n",
    "print('This model has been trained for', prev_cycles, 'epochs already')    \n",
    "file = lm_base_file + str(prev_cycles)\n",
    "learner_file = base_path/file\n",
    "learn.load(learner_file)\n",
    "learn.unfreeze()\n",
    "print('loaded existing learner from', str(learner_file))\n",
    "\n",
    "\n",
    "learn.fit_one_cycle(num_cycles, 1e-3, moms=(0.8,0.7))\n",
    "file = lm_base_file + str(prev_cycles + num_cycles + 1)\n",
    "learner_file = base_path/file\n",
    "learn.save(learner_file)\n",
    "release_mem()\n",
    "    \n",
    "print('completed', num_cycles, 'new training epochs')\n",
    "print('completed', num_cycles + prev_cycles, 'total training epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now testing with multiple epochs and learning rate of 0.001\n",
      "This model has been trained for 4 epochs already\n",
      "loaded existing learner from /home/seth/mimic/mimic_lm_fine_tuned_4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.719432</td>\n",
       "      <td>1.848607</td>\n",
       "      <td>0.624292</td>\n",
       "      <td>12:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.737586</td>\n",
       "      <td>1.826098</td>\n",
       "      <td>0.628307</td>\n",
       "      <td>12:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.610998</td>\n",
       "      <td>1.810599</td>\n",
       "      <td>0.632050</td>\n",
       "      <td>12:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.569480</td>\n",
       "      <td>1.811883</td>\n",
       "      <td>0.632433</td>\n",
       "      <td>12:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed 4 new training epochs\n",
      "completed 8 total training epochs\n",
      "now testing with multiple epochs and learning rate of 0.005\n",
      "This model has been trained for 4 epochs already\n",
      "loaded existing learner from /home/seth/mimic/mimic_lm_fine_tuned_4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.875421</td>\n",
       "      <td>1.969297</td>\n",
       "      <td>0.604634</td>\n",
       "      <td>12:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.944320</td>\n",
       "      <td>1.933842</td>\n",
       "      <td>0.609719</td>\n",
       "      <td>12:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.735338</td>\n",
       "      <td>1.828000</td>\n",
       "      <td>0.626465</td>\n",
       "      <td>12:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.597683</td>\n",
       "      <td>1.796420</td>\n",
       "      <td>0.633234</td>\n",
       "      <td>12:58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed 4 new training epochs\n",
      "completed 8 total training epochs\n",
      "now testing with multiple epochs and learning rate of 0.01\n",
      "This model has been trained for 4 epochs already\n",
      "loaded existing learner from /home/seth/mimic/mimic_lm_fine_tuned_4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.192642</td>\n",
       "      <td>2.164820</td>\n",
       "      <td>0.576094</td>\n",
       "      <td>12:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.147991</td>\n",
       "      <td>2.111975</td>\n",
       "      <td>0.583414</td>\n",
       "      <td>12:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.953963</td>\n",
       "      <td>1.922985</td>\n",
       "      <td>0.611166</td>\n",
       "      <td>12:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.779928</td>\n",
       "      <td>1.854817</td>\n",
       "      <td>0.623151</td>\n",
       "      <td>12:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed 4 new training epochs\n",
      "completed 8 total training epochs\n",
      "now testing with multiple epochs and learning rate of 0.05\n",
      "This model has been trained for 4 epochs already\n",
      "loaded existing learner from /home/seth/mimic/mimic_lm_fine_tuned_4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.263784</td>\n",
       "      <td>4.149484</td>\n",
       "      <td>0.380039</td>\n",
       "      <td>12:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10.339797</td>\n",
       "      <td>7.657226</td>\n",
       "      <td>0.057768</td>\n",
       "      <td>12:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.237694</td>\n",
       "      <td>4.974317</td>\n",
       "      <td>0.282988</td>\n",
       "      <td>12:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.493209</td>\n",
       "      <td>4.284114</td>\n",
       "      <td>0.388155</td>\n",
       "      <td>12:58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed 4 new training epochs\n",
      "completed 8 total training epochs\n",
      "now testing with multiple epochs and learning rate of 0.1\n",
      "This model has been trained for 4 epochs already\n",
      "loaded existing learner from /home/seth/mimic/mimic_lm_fine_tuned_4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.972384</td>\n",
       "      <td>4.839313</td>\n",
       "      <td>0.313887</td>\n",
       "      <td>12:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.960479</td>\n",
       "      <td>5.945019</td>\n",
       "      <td>0.075366</td>\n",
       "      <td>12:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.908723</td>\n",
       "      <td>5.903868</td>\n",
       "      <td>0.079820</td>\n",
       "      <td>12:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.928491</td>\n",
       "      <td>5.894521</td>\n",
       "      <td>0.079889</td>\n",
       "      <td>12:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed 4 new training epochs\n",
      "completed 8 total training epochs\n"
     ]
    }
   ],
   "source": [
    "num_cycles = 4\n",
    "prev_cycles = 4\n",
    "\n",
    "for lr in [1e-3, 5e-3, 1e-2, 5e-2, 1e-1]:\n",
    "    print('now testing with multiple epochs and learning rate of', lr)\n",
    "    print('This model has been trained for', prev_cycles, 'epochs already')    \n",
    "    file = lm_base_file + str(prev_cycles)\n",
    "    learner_file = base_path/file\n",
    "    learn.load(learner_file)\n",
    "    learn.unfreeze()\n",
    "    print('loaded existing learner from', str(learner_file))\n",
    "\n",
    "\n",
    "    learn.fit_one_cycle(num_cycles, lr, moms=(0.8,0.7))\n",
    "    file = lm_base_file + str(prev_cycles + num_cycles + 1)\n",
    "    learner_file = base_path/file\n",
    "    learn.save(learner_file)\n",
    "    release_mem()\n",
    "\n",
    "    print('completed', num_cycles, 'new training epochs')\n",
    "    print('completed', num_cycles + prev_cycles, 'total training epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 9:54:16 <p><table style='width:375px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "    <th>time</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>0</th>\n",
       "    <th>1.926960</th>\n",
       "    <th>1.832659</th>\n",
       "    <th>0.627496</th>\n",
       "    <th>1:14:14</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>1.808083</th>\n",
       "    <th>1.755725</th>\n",
       "    <th>0.637424</th>\n",
       "    <th>1:14:15</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>2</th>\n",
       "    <th>1.747903</th>\n",
       "    <th>1.697741</th>\n",
       "    <th>0.645431</th>\n",
       "    <th>1:14:15</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>3</th>\n",
       "    <th>1.714081</th>\n",
       "    <th>1.652703</th>\n",
       "    <th>0.652703</th>\n",
       "    <th>1:14:19</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>4</th>\n",
       "    <th>1.637801</th>\n",
       "    <th>1.602961</th>\n",
       "    <th>0.660170</th>\n",
       "    <th>1:14:15</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>5</th>\n",
       "    <th>1.596906</th>\n",
       "    <th>1.553225</th>\n",
       "    <th>0.668557</th>\n",
       "    <th>1:14:14</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>6</th>\n",
       "    <th>1.572020</th>\n",
       "    <th>1.519172</th>\n",
       "    <th>0.674477</th>\n",
       "    <th>1:14:26</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>7</th>\n",
       "    <th>1.517364</th>\n",
       "    <th>1.510010</th>\n",
       "    <th>0.676342</th>\n",
       "    <th>1:14:14</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jupyter/mimic/mimic_lm_fine_tuned_3.pth']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn_pattern = lm_base_file + '*'\n",
    "training_files = glob.glob(str(base_path/fn_pattern))\n",
    "training_files.sort()\n",
    "training_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to load the last file\n",
    "learn.load(training_files[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For confirmation, she underwent CTA of the lung which was negative for pulmonary embolism \n",
      "  but showed no PE , but did show some pulmonary edema . She has had \n",
      "  some mild dyspnea on exertion but has improved . She was brought to the \n",
      "  ED for further evaluation\n",
      "For confirmation, she underwent CTA of the lung which was negative for pulmonary embolism or dissection . \n",
      "  She was extubated today and given 1 unit of prbcs for Hct of 24 . She is \n",
      "  afebrile , HR in the 120s , BP stable . She is\n"
     ]
    }
   ],
   "source": [
    "# test the language generation capabilities of this model (not the point, but is interesting)\n",
    "TEXT = \"For confirmation, she underwent CTA of the lung which was negative for pulmonary embolism\"\n",
    "N_WORDS = 40\n",
    "N_SENTENCES = 2\n",
    "print(\"\\n\".join(learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder(enc_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the encoder:\n",
    "\n",
    "```python\n",
    "learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)\n",
    "learn.load_encoder(enc_file)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now based on our language model, train a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_file = 'mimic_cl.pickle'\n",
    "filename = base_path/class_file\n",
    "\n",
    "if os.path.isfile(filename):\n",
    "    data_cl = load_data(base_path, file, bs=bs)\n",
    "else:\n",
    "    data_cl = (TextList.from_df(df, cols='', vocab=data_lm.vocab)\n",
    "               #grab all the text files in path\n",
    "               .split_by_folder(valid='test')\n",
    "               #split by train and valid folder (that only keeps 'train' and 'test' so no need to filter)\n",
    "               .label_from_folder(classes=['neg', 'pos'])\n",
    "               #label them all with their folders\n",
    "               .databunch(bs=bs))\n",
    "\n",
    "data_cl.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-c42a15b2c7cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cl.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.CATEGORY.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1676"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.DESCRIPTION.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(filename):\n",
    "    data_lm = load_data(base_path, file, bs=bs)\n",
    "else:\n",
    "    data_lm = (TextList.from_df(df, 'texts.csv', cols='TEXT')\n",
    "               #df has several columns; actual text is in column TEXT\n",
    "               .split_by_rand_pct(valid_pct=valid_pct, seed=seed)\n",
    "               #We randomly split and keep 10% for validation\n",
    "               .label_from_df(cols='DESCRIPTION')\n",
    "               #We want to do a language model so we label accordingly\n",
    "               .databunch(bs=bs))\n",
    "    data_lm.save(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the version from the original example\n",
    "```python\n",
    "data_clas = (TextList.from_folder(path, vocab=data_lm.vocab)\n",
    "             #grab all the text files in path\n",
    "             .split_by_folder(valid='test')\n",
    "             #split by train and valid folder (that only keeps 'train' and 'test' so no need to filter)\n",
    "             .label_from_folder(classes=['neg', 'pos'])\n",
    "             #label them all with their folders\n",
    "             .databunch(bs=bs))\n",
    "\n",
    "data_clas.save('data_clas.pkl')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(data_cl, AWD_LSTM, drop_mult=0.5)\n",
    "learn.load_encoder(enc_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change learning rate based on results from the above plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, 2e-2, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
